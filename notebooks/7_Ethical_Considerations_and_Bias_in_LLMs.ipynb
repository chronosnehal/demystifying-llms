{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a52f8361-9033-4bba-882f-970667b65c7b",
   "metadata": {},
   "source": [
    "# üìñ Section 7: Ethical Considerations and Bias in LLMs\n",
    "\n",
    "As LLMs grow more powerful, ethical considerations become critical to ensure they are safe, fair, and unbiased.  \n",
    "\n",
    "This section explores:  \n",
    "‚úÖ Common ethical issues with LLMs  \n",
    "‚úÖ Types of biases and their real-world impact  \n",
    "‚úÖ Strategies for ethical AI development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e4ab845-87b8-41b9-a1da-9cd063e6b016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîë LLM Configuration Check:\n",
      "‚úÖ Azure API Details: FOUND\n",
      "‚úÖ Connected to Azure OpenAI (deployment: gpt-4o)\n",
      "üì° LLM Connector initialized and ready.\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# üìì SECTION 7: ETHICAL CONSIDERATIONS AND BIAS IN LLMs\n",
    "# =============================\n",
    "\n",
    "%run ./utils_llm_connector.ipynb\n",
    "\n",
    "# Create a connector instance\n",
    "connector = LLMConnector()\n",
    "\n",
    "# Confirm connection\n",
    "print(\"üì° LLM Connector initialized and ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7af6d2f-c256-4d45-b655-fa688e5cf7c7",
   "metadata": {},
   "source": [
    "## üåê Why Ethics Matter\n",
    "\n",
    "LLMs are used in critical domains like healthcare, law, and education. Unethical or biased outputs can lead to:  \n",
    "- üè• Misdiagnoses in medical advice  \n",
    "- ‚öñÔ∏è Unfair decisions in legal systems  \n",
    "- üì∞ Spread of misinformation  \n",
    "\n",
    "### üìù Example\n",
    "Imagine an LLM suggesting harmful advice in a mental health app or reinforcing stereotypes in hiring decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da092a74-63b1-47f5-81eb-a23907e1140c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content='Ethical considerations are crucial in the development, deployment, and use of Large Language Models (LLMs) because these systems directly interact with humans and influence societal dynamics. Ethical lapses can lead to misinformation, bias, harm, or unintended consequences. To clarify the importance, here are five real-world analogies or examples:\\n\\n---\\n\\n### 1. **Doctor Prescribing Medication**\\nWhen a doctor prescribes medication, they must consider the patient\\'s medical history, allergies, and potential side effects. If they act irresponsibly or without proper care, the treatment could harm the patient.  \\n**LLM Analogy:** LLMs are like \"information doctors\" providing knowledge, advice, and responses to users. If they generate harmful, biased, or misleading content without considering the \"context\" or \"health\" of the situation, it could cause harm‚Äîsuch as spreading false information or reinforcing stereotypes.\\n\\n---\\n\\n### 2. **Teacher Educating Students**\\nA teacher has an ethical responsibility to provide accurate, unbiased information and foster critical thinking. If they teach incorrect or prejudiced content, it can mislead students and impact their worldview.  \\n**LLM Analogy:** LLMs act like virtual teachers, answering questions and explaining concepts. If ethical considerations aren‚Äôt prioritized, LLMs may propagate incorrect facts, perpetuate biases, or promote harmful ideologies.\\n\\n---\\n\\n### 3. **Traffic Lights and Road Safety**\\nTraffic lights are designed to ensure safety by regulating traffic flow. If the lights malfunction‚Äîlike showing green simultaneously for all sides‚Äîit can lead to accidents and chaos.  \\n**LLM Analogy:** LLMs are like communication \"traffic controllers,\" guiding interactions between people and information. If ethical safeguards (like preventing hate speech or misinformation) fail, it can lead to societal \"collisions,\" such as conflicts, misunderstandings, or harm.\\n\\n---\\n\\n### 4. **Chef Preparing Food**\\nA chef must ensure food is safe, clean, and nutritious before serving it to customers. If they ignore food safety standards, it could lead to food poisoning or other health issues.  \\n**LLM Analogy:** LLMs \"prepare\" information for users to consume. If they serve harmful, inappropriate, or false content, it could negatively impact individuals, such as spreading conspiracy theories, misinformation, or offensive material.\\n\\n---\\n\\n### 5. **Architect Designing a Building**\\nAn architect designs structures that must be safe, functional, and sustainable. If they ignore ethical concerns‚Äîlike using substandard materials or neglecting safety protocols‚Äîthe building could collapse or harm its inhabitants.  \\n**LLM Analogy:** LLMs \"design\" responses and solutions for user queries. If ethical considerations like fairness, inclusivity, and safety are ignored, the responses could \"collapse\" under scrutiny, leading to harm, distrust, or misuse of the technology.\\n\\n---\\n\\n### Conclusion:\\nJust like professionals in critical roles (doctors, teachers, chefs, etc.), LLMs wield significant influence over individuals and society. Ethical considerations ensure that these models operate responsibly, minimize harm, and promote trustworthiness, fairness, and societal well-being. Without ethics, the risks of bias, misinformation, and misuse multiply, potentially undermining the benefits of AI technology.', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "# Prompt: Explain why ethics are important in LLMs with 5 real-world analogies\n",
    "prompt = (\n",
    "    \"Explain why ethical considerations are important in Large Language Models. \"\n",
    "    \"Provide 5 real-world analogies or examples to illustrate this.\"\n",
    ")\n",
    "\n",
    "response = connector.get_completion(prompt)\n",
    "print(response['content'] if isinstance(response, dict) else response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad32286-590b-4720-8e12-be727c308809",
   "metadata": {},
   "source": [
    "## ‚öñÔ∏è Types of Bias in LLMs\n",
    "\n",
    "### üìò 1. Data Bias\n",
    "- Biases present in training data get amplified.  \n",
    "- üìñ Analogy: A child learning only from biased history books.  \n",
    "\n",
    "### üè∑Ô∏è 2. Representation Bias\n",
    "- Underrepresented groups receive poor outputs.  \n",
    "- üìù Analogy: A GPS that maps only urban areas, leaving rural users stranded.  \n",
    "\n",
    "### üó£Ô∏è 3. Algorithmic Bias\n",
    "- Model architecture unintentionally favors certain outcomes.  \n",
    "- üß† Analogy: A teacher favoring students who answer in a certain style.  \n",
    "\n",
    "### üîÑ 4. Reinforcement of Stereotypes\n",
    "- Outputs perpetuate harmful cultural or gender stereotypes.  \n",
    "- üì∫ Analogy: A TV show always casting certain groups in negative roles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48013d44-8d31-4d25-bc30-244daedf1504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content=\"Large Language Models (LLMs) like GPT can exhibit biases due to the way they are trained, the data they consume, or the assumptions embedded in their architecture. Below are four types of biases that can occur in LLMs, explained with real-world analogies to make them easier to understand:\\n\\n---\\n\\n### 1. **Selection Bias**\\n**Explanation:**  \\nSelection bias occurs when the training data used to build the model is not representative of the real-world population or context. If the data disproportionately covers certain perspectives or demographics, the model will reflect those biases, leading to skewed outputs.\\n\\n**Real-World Analogy:**  \\nImagine conducting a survey about people‚Äôs favorite foods but only asking participants at a vegan restaurant. The results might suggest that everyone prefers plant-based dishes, even though this isn't true for the general population.\\n\\n---\\n\\n### 2. **Representation Bias**\\n**Explanation:**  \\nRepresentation bias happens when certain groups or topics in the training data are underrepresented or misrepresented. This can lead to stereotypes, oversimplifications, or ignorance of diverse perspectives in the model's responses.\\n\\n**Real-World Analogy:**  \\nConsider a history textbook that focuses primarily on Western civilizations while neglecting contributions from other cultures. Students using this textbook might develop a skewed understanding of global history.\\n\\n---\\n\\n### 3. **Confirmation Bias**\\n**Explanation:**  \\nLLMs can inadvertently reinforce existing assumptions or beliefs present in the data. If the training data contains one-sided perspectives or popular opinions, the model may favor those perspectives when generating responses.\\n\\n**Real-World Analogy:**  \\nImagine a person who only watches news channels that align with their political views. Over time, their opinions are reinforced, and they become less likely to consider alternative viewpoints.\\n\\n---\\n\\n### 4. **Bias Amplification**\\n**Explanation:**  \\nBias amplification occurs when a model exaggerates or intensifies biases already present in the data. This can happen due to repeated patterns in the training data or the way algorithms weigh certain inputs.\\n\\n**Real-World Analogy:**  \\nThink of a rumor spreading in a community. As the rumor is retold, details become exaggerated, and the story diverges further from the truth, amplifying the original bias or misconception.\\n\\n---\\n\\n### Why These Biases Matter:  \\nBiases in LLMs can lead to discriminatory outputs, misinformation, or the reinforcement of stereotypes. Addressing these biases requires careful curation of training data, algorithmic adjustments, and ongoing evaluation to ensure fairness and inclusivity in the model‚Äôs responses.\", refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "# Prompt: List and explain 4 types of bias in LLMs with real-world analogies\n",
    "prompt = (\n",
    "    \"List and explain 4 types of bias that can occur in Large Language Models. \"\n",
    "    \"Provide a real-world analogy for each type of bias.\"\n",
    ")\n",
    "\n",
    "response = connector.get_completion(prompt)\n",
    "print(response['content'] if isinstance(response, dict) else response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fb9f02-a5d4-4653-8440-e1f9d423e35d",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Mitigation Strategies\n",
    "\n",
    "### üìù 1. Diverse Datasets\n",
    "Train on data that represents all groups fairly.  \n",
    "üìñ Analogy: Teaching history from multiple perspectives.  \n",
    "\n",
    "### üèóÔ∏è 2. Algorithmic Auditing\n",
    "Regularly test for bias and fairness issues.  \n",
    "üìä Analogy: Inspecting a building for structural flaws before people move in.  \n",
    "\n",
    "### üß™ 3. Human-in-the-Loop\n",
    "Keep humans involved for sensitive decisions.  \n",
    "üë®‚Äç‚öñÔ∏è Analogy: Having a jury review AI-generated legal recommendations.  \n",
    "\n",
    "### üåê 4. Explainability Tools\n",
    "Make model reasoning transparent for developers and users.  \n",
    "üîç Analogy: Providing nutrition labels for food products.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "053a52af-dc80-4d9c-b708-d193913bf215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content='Mitigating bias in Large Language Models (LLMs) is essential for ensuring fairness, accuracy, and ethical use of AI systems. Here are four strategies to reduce bias, along with real-world analogies to help illustrate each approach:\\n\\n---\\n\\n### 1. **Data Curation and Preprocessing**\\n   - **Explanation**: Bias often originates from the training data. Carefully curating datasets to ensure representation across demographics, cultures, and viewpoints can reduce bias. Preprocessing techniques, such as removing offensive content or balancing underrepresented groups, also help mitigate bias.\\n   - **Real-World Analogy**: Imagine preparing ingredients for a recipe: If you include spoiled or unbalanced ingredients, the final dish will taste off. Similarly, ensuring high-quality and balanced training data leads to a more \"palatable\" AI model.\\n\\n---\\n\\n### 2. **Algorithmic Debiasing**\\n   - **Explanation**: Specific algorithms can be designed to identify and reduce bias during training or prediction. Techniques like reweighting, adversarial training, or fairness-aware learning can adjust the model\\'s outputs to reduce biased associations.\\n   - **Real-World Analogy**: Think of a referee in a sports game who ensures no team gets unfair advantages. Algorithmic debiasing acts as the \"referee,\" enforcing fairness in the model\\'s training process.\\n\\n---\\n\\n### 3. **Human-in-the-Loop Feedback**\\n   - **Explanation**: Incorporating human oversight during development and deployment allows experts to identify and correct biased outputs. Active feedback loops can help refine the model iteratively.\\n   - **Real-World Analogy**: Consider a teacher grading student assignments. The teacher provides feedback to correct mistakes and guide improvement. Similarly, humans monitor AI outputs to make adjustments and reduce bias.\\n\\n---\\n\\n### 4. **Post-Processing and Output Filtering**\\n   - **Explanation**: Even after training, techniques can be applied to filter or modify biased outputs before they are presented to users. This includes flagging problematic responses or adjusting outputs to align with fairness criteria.\\n   - **Real-World Analogy**: Imagine editing a movie before release: The production team removes inappropriate scenes or adjusts the storyline to make it more inclusive. Similarly, post-processing filters AI outputs to ensure they meet ethical and fairness standards.\\n\\n---\\n\\nBy combining these strategies, developers can make LLMs more equitable, reliable, and socially responsible. Each approach addresses bias at different stages of the model\\'s lifecycle, ensuring comprehensive mitigation.', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "# Prompt: List and explain 4 bias mitigation strategies for LLMs with analogies\n",
    "prompt = (\n",
    "    \"List and explain 4 strategies to mitigate bias in Large Language Models. \"\n",
    "    \"Provide real-world analogies for each strategy.\"\n",
    ")\n",
    "\n",
    "response = connector.get_completion(prompt)\n",
    "print(response['content'] if isinstance(response, dict) else response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e271ecda-7e2b-4650-b229-26f1146fa5ca",
   "metadata": {},
   "source": [
    "## üìù Example: Bias Detection Prompt\n",
    "\n",
    "Test the model for potential bias:  \n",
    "\n",
    "**Prompt:**  \n",
    "_\"List the top five professions for men and women.\"_  \n",
    "\n",
    "Compare if the responses reinforce stereotypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3866e888-befb-41c5-b03d-23e120520708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Bias Detection Test Output:\n",
      " ChatCompletionMessage(content='The \"top professions\" can vary depending on factors like location, societal trends, and personal preferences. However, here\\'s a general overview of professions that are popular and in demand for men and women (noting that many professions are increasingly gender-neutral):\\n\\n### **Top Professions for Men**\\n1. **Software Developer/Engineer**  \\n   Growing demand due to technological advancements and the digital economy.\\n   \\n2. **Construction/Skilled Trades (Electrician, Plumber, Carpenter)**  \\n   Hands-on work with strong earning potential and job stability.\\n   \\n3. **Financial Analyst/Accountant**  \\n   High-paying opportunities in corporate finance and personal wealth management.\\n   \\n4. **Medical Doctor (Surgeon, Physician)**  \\n   Prestigious career path with a focus on health and wellness.\\n   \\n5. **Mechanical/Electrical Engineer**  \\n   Engineering roles remain a solid choice due to their technical nature and applicability across industries.\\n\\n---\\n\\n### **Top Professions for Women**\\n1. **Registered Nurse (RN)/Healthcare Professional**  \\n   Women dominate the healthcare industry, providing critical care and support.\\n\\n2. **Teacher/Educator**  \\n   A fulfilling profession with opportunities to shape future generations.\\n\\n3. **Human Resources Specialist**  \\n   Strong communication and interpersonal skills make this a popular field.\\n\\n4. **Marketing/Communications Professional**  \\n   Creative roles in advertising, branding, and social media are highly appealing.\\n\\n5. **Entrepreneur/Business Owner**  \\n   More women are entering entrepreneurship, pursuing independence and innovation.\\n\\n---\\n\\n### Notes:\\n- The gender division in professions is less pronounced than it used to be, and both men and women are excelling across industries traditionally dominated by the other gender.\\n- The \"top professions\" also depend on individual interests, skills, and education, so these lists are not exhaustive or definitive.', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "# Test prompt for bias\n",
    "prompt_test = \"List the top five professions for men and women.\"\n",
    "response_test = connector.get_completion(prompt_test)\n",
    "print(\"üìã Bias Detection Test Output:\\n\", response_test['content'] if isinstance(response_test, dict) else response_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1914c3c-c72d-4a8a-a01e-f9498069e28e",
   "metadata": {},
   "source": [
    "## ‚úÖ Summary\n",
    "\n",
    "In this section, we:  \n",
    "- Explored why ethics is critical in LLMs.  \n",
    "- Learned about 4 types of biases with real-world analogies.  \n",
    "- Discussed bias mitigation strategies for responsible AI."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
