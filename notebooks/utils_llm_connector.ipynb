{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5555c29-ac36-4b15-a421-47543d7dd944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# üîß CONFIGURATION\n",
    "# =============================\n",
    "\n",
    "# Choose LLM Provider: \"openai\" or \"azure\"\n",
    "LLM_PROVIDER = \"azure\"  # Change to \"azure\" for Azure OpenAI\n",
    "\n",
    "# =============================\n",
    "# üóùÔ∏è API Keys & Endpoints\n",
    "# =============================\n",
    "\n",
    "# OpenAI Settings\n",
    "OPENAI_API_KEY = \"<Your OpenAI Key Here>\"  # <-- Set your OpenAI API Key here or leave blank for .env\n",
    "\n",
    "# Azure OpenAI Settings\n",
    "AZURE_API_KEY = \"<Your Azure OpenAI Key Here>\"  # <-- Set your Azure API Key here or leave blank for .env\n",
    "AZURE_ENDPOINT = \"<Your Azure OpenAI Endpoint Here>\"  # <-- Your Azure endpoint\n",
    "\n",
    "# =============================\n",
    "# üì¶ Model and Deployment\n",
    "# =============================\n",
    "\n",
    "# OpenAI model\n",
    "OPENAI_MODEL = \"gpt-4o\"\n",
    "\n",
    "# Azure OpenAI deployment & API version\n",
    "AZURE_MODEL = \"gpt-4o\"\n",
    "AZURE_DEPLOYMENT = \"gpt-4o\"  # <-- Azure deployment name\n",
    "AZURE_API_VERSION = \"2024-10-21\"  # Example API version\n",
    "\n",
    "# Temperature setting (controls creativity)\n",
    "TEMPERATURE = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc410f57d12c478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîë LLM Configuration Check:\n",
      "‚úÖ Azure API Details: FOUND\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# üì¶ IMPORTS & ENV LOADER\n",
    "# =============================\n",
    "from langchain_openai import ChatOpenAI, AzureChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Fallback to .env values if not set in config\n",
    "OPENAI_API_KEY = OPENAI_API_KEY or os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "AZURE_API_KEY = AZURE_API_KEY or os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "AZURE_ENDPOINT = AZURE_ENDPOINT or os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "AZURE_API_VERSION = AZURE_API_VERSION or os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2024-07-01-preview\")\n",
    "AZURE_DEPLOYMENT = AZURE_DEPLOYMENT or os.getenv(\"AZURE_OPENAI_DEPLOYMENT\")\n",
    "\n",
    "# =============================\n",
    "# ‚úÖ VALIDATION\n",
    "# =============================\n",
    "def validate_keys_and_endpoints():\n",
    "    print(\"üîë LLM Configuration Check:\")\n",
    "    if LLM_PROVIDER == \"openai\":\n",
    "        if OPENAI_API_KEY:\n",
    "            print(f\"‚úÖ OpenAI API Details: FOUND\")\n",
    "        else:\n",
    "            print(\"‚ùå OpenAI API Key or Endpoint MISSING!\")\n",
    "    elif LLM_PROVIDER == \"azure\":\n",
    "        if AZURE_API_KEY and AZURE_ENDPOINT and AZURE_API_VERSION and AZURE_DEPLOYMENT:\n",
    "            print(f\"‚úÖ Azure API Details: FOUND\")\n",
    "        else:\n",
    "            print(\"‚ùå Azure API Key, Endpoint, Deployment, or API Version MISSING!\")\n",
    "    else:\n",
    "        print(\"‚ùå Invalid LLM_PROVIDER selected. Must be 'openai' or 'azure'.\")\n",
    "\n",
    "# Run validation\n",
    "validate_keys_and_endpoints()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "371ffe4e-a2e3-4939-855f-e5e12026df26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# üß† LLMConnector CLASS\n",
    "# =============================\n",
    "from openai import OpenAI, AzureOpenAI\n",
    "\n",
    "class LLMConnector:\n",
    "    def __init__(self, provider=LLM_PROVIDER, model=None, temperature=TEMPERATURE):\n",
    "        \"\"\"\n",
    "        provider: 'openai' or 'azure'\n",
    "        model: Model name (e.g., gpt-4 or gpt-35-turbo)\n",
    "        temperature: Sampling temperature for responses\n",
    "        \"\"\"\n",
    "        self.provider = provider.lower()\n",
    "        self.model = model\n",
    "        self.temperature = temperature\n",
    "        self.client = None\n",
    "        self._configure_client()\n",
    "\n",
    "    def _configure_client(self):\n",
    "        if self.provider == \"openai\":\n",
    "            self.model = OPENAI_MODEL\n",
    "            if not OPENAI_API_KEY:\n",
    "                raise ValueError(\"‚ùå OpenAI API Key is missing!\")\n",
    "            self.client = OpenAI(\n",
    "                api_key=OPENAI_API_KEY\n",
    "            )\n",
    "            print(f\"‚úÖ Connected to OpenAI (model: {self.model or OPENAI_MODEL})\")\n",
    "        \n",
    "        elif self.provider == \"azure\":\n",
    "            if not AZURE_API_KEY or not AZURE_ENDPOINT or not AZURE_DEPLOYMENT:\n",
    "                raise ValueError(\"‚ùå Azure OpenAI configuration missing!\")\n",
    "            self.client = AzureOpenAI(\n",
    "                api_key=AZURE_API_KEY,\n",
    "                azure_endpoint=AZURE_ENDPOINT,\n",
    "                azure_deployment=AZURE_DEPLOYMENT,\n",
    "                api_version=AZURE_API_VERSION\n",
    "            )\n",
    "            print(f\"‚úÖ Connected to Azure OpenAI (deployment: {AZURE_DEPLOYMENT})\")\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(\"‚ùå Invalid provider. Use 'openai' or 'azure'.\")\n",
    "\n",
    "    def get_completion(self, prompt, max_tokens=None):\n",
    "        \"\"\"\n",
    "        Generate a completion for a given prompt.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if self.provider == \"openai\":\n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=self.model,\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                    max_tokens=max_tokens,\n",
    "                    temperature=self.temperature\n",
    "                )\n",
    "            elif self.provider == \"azure\":\n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=self.model,\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                    max_tokens=max_tokens,\n",
    "                    temperature=self.temperature\n",
    "                )\n",
    "            return response.choices[0].message\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error during completion: {e}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f9f3d92-af9b-4510-8580-713336d27f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # =============================\n",
    "# # üöÄ EXAMPLE USAGE\n",
    "# # =============================\n",
    "\n",
    "# # Toggle Debug Mode: Set to True to see raw JSON response\n",
    "# DEBUG_MODE = False\n",
    "\n",
    "# # üîÑ Create a connector instance (uses global LLM_PROVIDER)\n",
    "# connector = LLMConnector()\n",
    "\n",
    "# # üìù Define your prompt\n",
    "# prompt = \"Explain the key differences between LLM training and inference in simple terms.\"\n",
    "\n",
    "# # üì° Get a response\n",
    "# response = connector.get_completion(prompt)\n",
    "\n",
    "# # üìã Handle and display response\n",
    "# if response:\n",
    "#     if DEBUG_MODE:\n",
    "#         # üõ†Ô∏è Print full raw response JSON\n",
    "#         print(\"üîé Raw API Response:\")\n",
    "#         from pprint import pprint\n",
    "#         pprint(response)\n",
    "#     else:\n",
    "#         # üí¨ Print only the response text\n",
    "#         print(\"üí¨ LLM Response:\")\n",
    "#         print(response['content'] if isinstance(response, dict) else response)\n",
    "# else:\n",
    "#     print(\"‚ö†Ô∏è No response received.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
