{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3bf616d-1030-4318-a52a-565c5780c29a",
   "metadata": {},
   "source": [
    "# üìñ Section 6: Evaluation and Metrics for LLMs\n",
    "\n",
    "Evaluating LLMs involves assessing how well they understand, reason, and generate language.  \n",
    "\n",
    "This section will explore:  \n",
    "‚úÖ Common evaluation metrics for LLMs  \n",
    "‚úÖ Why evaluation is complex  \n",
    "‚úÖ Real-world examples and analogies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90c35335-8110-43d8-a6b0-2adba7690765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîë LLM Configuration Check:\n",
      "‚úÖ Azure API Details: FOUND\n",
      "‚úÖ Connected to Azure OpenAI (deployment: gpt-4o)\n",
      "üì° LLM Connector initialized and ready.\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# üìì SECTION 6: EVALUATION AND METRICS FOR LLMs\n",
    "# =============================\n",
    "\n",
    "%run ./utils_llm_connector.ipynb\n",
    "\n",
    "# Create a connector instance\n",
    "connector = LLMConnector()\n",
    "\n",
    "# Confirm connection\n",
    "print(\"üì° LLM Connector initialized and ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337e2d4f-a007-41d0-9d75-0bd04534391d",
   "metadata": {},
   "source": [
    "## üìä Why Evaluate LLMs?\n",
    "\n",
    "LLMs are evaluated to ensure:  \n",
    "- ‚úÖ Quality of responses  \n",
    "- ‚úÖ Reliability and safety  \n",
    "- ‚úÖ Domain-specific performance  \n",
    "\n",
    "Without proper evaluation, we risk deploying models that hallucinate, are biased, or perform poorly in real-world applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f54bb5d0-5650-420f-a28f-5fbbc27da086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content='Evaluating Large Language Models (LLMs) is crucial because it helps us understand their strengths, limitations, and suitability for specific tasks. Without proper evaluation, we risk deploying models that may perform poorly, behave unpredictably, or cause harm in critical applications. Below are five real-world analogies to illustrate the importance of evaluation:\\n\\n---\\n\\n### 1. **Test-Driving a Car Before Buying**\\nBefore purchasing a new car, you take it for a test drive to assess its performance, handling, and safety features. If you skip this step, you risk buying a car that may not meet your needs or could even be dangerous to use. Similarly, evaluating LLMs ensures they function as intended and meet the requirements of the task they‚Äôre designed for, such as generating accurate and ethical responses.\\n\\n---\\n\\n### 2. **Quality Control in Manufacturing**\\nFactories rigorously test products, like electronics or food items, to ensure they meet safety and quality standards before reaching consumers. If quality control is neglected, defective products could harm users or fail to function properly. In the same way, evaluating LLMs ensures that they are reliable, accurate, and safe before being deployed in applications like customer support or healthcare.\\n\\n---\\n\\n### 3. **Examining a Recipe Before Cooking**\\nWhen cooking a new dish, you review the recipe to check if you have the right ingredients and equipment. Without this evaluation, you might end up with a dish that tastes bad or is incomplete. Evaluating LLMs is like reviewing the recipe‚Äîit ensures the model is capable of producing useful and coherent outputs for the tasks it‚Äôs assigned.\\n\\n---\\n\\n### 4. **Hiring an Employee**\\nBefore hiring someone, employers conduct interviews and reviews to evaluate their qualifications, skills, and suitability for the role. Skipping this process could result in hiring someone unqualified, leading to inefficiency or mistakes. Similarly, evaluating LLMs ensures they have the \"skills\" (e.g., language understanding, reasoning) required for the tasks they‚Äôll perform.\\n\\n---\\n\\n### 5. **Checking a Map Before a Road Trip**\\nBefore starting a long road trip, you check the map or GPS for the best route. If you don‚Äôt evaluate the directions, you could end up lost or delayed. Evaluating LLMs is similar‚Äîit helps confirm that the model is \"on the right path\" for generating accurate and contextually appropriate outputs, avoiding errors or inefficiencies.\\n\\n---\\n\\nIn all these analogies, the underlying principle is that proper evaluation prevents risks, ensures reliability, and optimizes performance, which is equally important for LLMs in real-world applications.', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "# Prompt: Explain why evaluating LLMs is important with 5 real-world analogies\n",
    "prompt = (\n",
    "    \"Explain why evaluating Large Language Models (LLMs) is important. \"\n",
    "    \"Provide 5 real-world analogies for better understanding.\"\n",
    ")\n",
    "\n",
    "response = connector.get_completion(prompt)\n",
    "print(response['content'] if isinstance(response, dict) else response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371266ce-8460-42ea-b208-2c3eed4bde61",
   "metadata": {},
   "source": [
    "## üìè Common Evaluation Metrics\n",
    "\n",
    "### üìù 1. Perplexity\n",
    "- Measures how well the model predicts the next word.\n",
    "- Lower is better.\n",
    "- üì¶ Analogy: Like a student's confidence in answering a quiz.\n",
    "\n",
    "### üìù 2. BLEU (Bilingual Evaluation Understudy)\n",
    "- Compares model output with reference text.\n",
    "- Higher means closer match.\n",
    "- üìñ Analogy: Like comparing student essays to a perfect answer key.\n",
    "\n",
    "### üìù 3. ROUGE (Recall-Oriented Understudy for Gisting Evaluation)\n",
    "- Measures overlap between generated and reference summaries.\n",
    "- üìÑ Analogy: Like checking how many key points a summary captures.\n",
    "\n",
    "### üìù 4. Accuracy\n",
    "- Used for classification tasks (e.g., sentiment analysis).\n",
    "- üéØ Analogy: Like measuring how often a dart hits the bullseye.\n",
    "\n",
    "### üìù 5. Human Evaluation\n",
    "- Judges assess quality based on fluency, relevance, and safety.\n",
    "- üë©‚Äç‚öñÔ∏è Analogy: Like food critics rating a chef‚Äôs dish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "371f1a9c-b499-4f1c-882b-d2ce89e130fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content='Certainly! Here are five common evaluation metrics used to assess the performance of Large Language Models (LLMs), along with real-world analogies to help explain each:\\n\\n---\\n\\n### 1. **Perplexity**\\n   - **Explanation**: Perplexity measures how well a language model predicts a sequence of words. It‚Äôs essentially the model‚Äôs confidence in generating text that matches the patterns seen in the training data. Lower perplexity indicates better performance, as it suggests the model is better at estimating probabilities for the next word in a sequence.\\n   - **Analogy**: Imagine a weather forecaster. If they consistently predict the right weather (e.g., \"It will rain today\"), their predictions are reliable. Perplexity is like measuring how \"perplexed\" the forecaster is when making predictions‚Äîlower perplexity means they\\'re confident and accurate about the forecast.\\n\\n---\\n\\n### 2. **BLEU (Bilingual Evaluation Understudy)**\\n   - **Explanation**: BLEU is a metric for comparing generated text (e.g., translations) to reference text. It calculates how many overlapping n-grams (word sequences) exist between the model\\'s output and the ground truth. Higher BLEU scores indicate closer alignment with the reference.\\n   - **Analogy**: Think of grading a student‚Äôs essay. If the essay uses the same key phrases and structure as the teacher‚Äôs example essay, it gets a high grade. BLEU measures how much the model‚Äôs output \"copies\" or aligns with the reference text.\\n\\n---\\n\\n### 3. **ROUGE (Recall-Oriented Understudy for Gisting Evaluation)**\\n   - **Explanation**: ROUGE evaluates the quality of summaries by comparing the overlap of words or phrases (n-grams) between the generated summary and a reference summary. It emphasizes recall, measuring how much of the reference content is captured in the generated summary.\\n   - **Analogy**: Imagine summarizing a movie plot for a friend. If your summary includes all the key scenes and themes your friend remembers, they‚Äôll say you did a good job. ROUGE checks how much of the original content is \"remembered\" in the summary.\\n\\n---\\n\\n### 4. **F1 Score**\\n   - **Explanation**: The F1 Score balances precision (how many relevant results are returned) and recall (how many relevant results were found out of the total possible). It‚Äôs commonly used for tasks like classification or identifying specific entities in text.\\n   - **Analogy**: Think of a talent scout searching for actors in a crowd. Precision is how many of the chosen actors are actually talented, while recall is how many talented actors were spotted overall. The F1 Score ensures the scout achieves a good balance of both.\\n\\n---\\n\\n### 5. **Human Evaluation**\\n   - **Explanation**: Human evaluators assess the model‚Äôs outputs based on criteria like fluency, relevance, coherence, and creativity. This subjective metric is used to judge how well the model performs in real-world scenarios, often complementing automated metrics.\\n   - **Analogy**: Think of a chef cooking a new dish. While the ingredients might follow a recipe (like automated metrics), the ultimate test is whether diners enjoy the taste and presentation. Human evaluation captures this subjective \"taste test.\"\\n\\n---\\n\\n### Bonus: **Exact Match (EM)**\\n   - **Explanation**: Exact Match measures whether the model‚Äôs output matches the ground truth completely. It‚Äôs often used in tasks like question answering, where the goal is to produce a precise response.\\n   - **Analogy**: Imagine a quiz where the answers must exactly match the teacher\\'s key. If you answer \"4\" to a math problem and the correct answer is also \"4,\" you get full credit. Exact Match rewards complete correctness.\\n\\n---\\n\\nThese metrics collectively help researchers and developers understand the strengths and weaknesses of LLMs and improve their performance across various tasks.', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "# Prompt: List 5 common evaluation metrics for LLMs with analogies\n",
    "prompt = (\n",
    "    \"List and explain 5 common evaluation metrics for Large Language Models. \"\n",
    "    \"Provide real-world analogies for each metric.\"\n",
    ")\n",
    "\n",
    "response = connector.get_completion(prompt)\n",
    "print(response['content'] if isinstance(response, dict) else response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5721d7be-bbf8-4866-983e-5b1921c40058",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è Challenges in Evaluation\n",
    "\n",
    "1. **Subjectivity**: What is ‚Äúgood‚Äù can vary across users.  \n",
    "2. **Context Sensitivity**: A response may be correct in one context but not another.  \n",
    "3. **Hallucinations**: Hard to detect automatically.  \n",
    "4. **Scalability**: Human evaluations don‚Äôt scale for billions of requests.  \n",
    "5. **Bias Detection**: Subtle biases are tricky to quantify.  \n",
    "\n",
    "### üìù Analogy\n",
    "Like grading creative writing essays‚Äîthere‚Äôs no single ‚Äúcorrect‚Äù answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a87c6736-7e91-4c62-9594-7ad90041ab06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content='Evaluating Large Language Models (LLMs) is a complex task, as it requires understanding their capabilities, limitations, and performance across diverse contexts. Here are five key challenges in evaluation, along with real-world analogies to help illustrate each:\\n\\n---\\n\\n### 1. **Defining \"Good\" Performance**\\n   - **Challenge:** Determining what constitutes \"good\" performance depends on the specific use case. For conversational AI, fluency and relevance matter, but for factual tasks, accuracy is critical. Different stakeholders might prioritize different metrics.\\n   - **Analogy:** **Judging a chef\\'s cooking skills.** Some diners may value presentation, others might prioritize flavor, while still others care about nutritional content. A single dish can\\'t satisfy all criteria equally.\\n\\n---\\n\\n### 2. **Handling Ambiguity in Responses**\\n   - **Challenge:** LLMs often generate responses that are plausible but ambiguous or vague. Evaluating whether such responses are \"acceptable\" depends on the context and the evaluator\\'s interpretation.\\n   - **Analogy:** **Assessing a weather forecast.** If you‚Äôre told, \"It might rain,\" it‚Äôs hard to say whether the forecast was useful‚Äîit depends on whether you needed detailed certainty or just general advice.\\n\\n---\\n\\n### 3. **Measuring Generalization Across Domains**\\n   - **Challenge:** LLMs are trained on diverse data, but their ability to generalize to unfamiliar domains or languages can vary. Evaluating this requires extensive testing across a broad set of examples.\\n   - **Analogy:** **Testing a car‚Äôs off-road capability.** A car might perform well on highways, but how it handles rocky trails, icy roads, or sand dunes requires distinct tests that mimic real-world conditions.\\n\\n---\\n\\n### 4. **Detecting Subtle Errors**\\n   - **Challenge:** LLMs can produce outputs that seem correct but include subtle factual inaccuracies, logical flaws, or ethical biases. Spotting these errors often requires deep expertise.\\n   - **Analogy:** **Proofreading a novel for typos.** While it\\'s easy to spot glaring mistakes, catching subtle errors like misplaced commas or slightly incorrect dates requires careful attention and domain knowledge.\\n\\n---\\n\\n### 5. **Evaluating Long-Term Consistency**\\n   - **Challenge:** LLMs may perform well at first but fail to maintain consistency over long interactions, especially in tasks requiring memory or reasoning. Evaluating this requires sustained testing over extended sequences.\\n   - **Analogy:** **Judging a marathon runner.** A runner might sprint well at the start but struggle to maintain pace or form over the entire race. Consistency over time is key to understanding performance.\\n\\n---\\n\\nBy addressing these challenges, researchers and developers can better understand the strengths and limitations of LLMs, ensuring their applications are both reliable and aligned with user needs.', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "# Prompt: List 5 challenges in evaluating LLMs with examples\n",
    "prompt = (\n",
    "    \"List 5 challenges in evaluating Large Language Models. \"\n",
    "    \"Provide a real-world analogy for each challenge.\"\n",
    ")\n",
    "\n",
    "response = connector.get_completion(prompt)\n",
    "print(response['content'] if isinstance(response, dict) else response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293af0a3-c4bb-4ead-8343-f504bbca17aa",
   "metadata": {},
   "source": [
    "## üìù Example: Simple Perplexity Approximation\n",
    "\n",
    "While true perplexity requires access to model internals, you can approximate it by asking the LLM to predict the next word in incomplete sentences and measuring its confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7297a4b5-b230-4b6f-927a-d3b17d3d7975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content=\"'The capital of France is Paris.'\\n\\nI am highly confident in this response because Paris is universally recognized as the capital of France. It is a well-documented fact in geography, history, and political studies, and Paris serves as the administrative, cultural, and economic center of France. This information is consistent across reliable sources and has been unchanged for centuries.\", refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "# Example: Asking model to predict next word\n",
    "prompt = (\n",
    "    \"Complete the following sentence and explain your confidence: \"\n",
    "    \"'The capital of France is'\"\n",
    ")\n",
    "\n",
    "response = connector.get_completion(prompt)\n",
    "print(response['content'] if isinstance(response, dict) else response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d57e0a-79a4-4fa5-92d0-a64a95402b1f",
   "metadata": {},
   "source": [
    "## ‚úÖ Summary\n",
    "\n",
    "In this section, we:  \n",
    "- Learned why LLM evaluation is essential.  \n",
    "- Explored 5 common evaluation metrics with real-world analogies.  \n",
    "- Discussed challenges unique to evaluating LLMs.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
