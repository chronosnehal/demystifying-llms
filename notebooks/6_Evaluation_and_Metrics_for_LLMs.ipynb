{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3bf616d-1030-4318-a52a-565c5780c29a",
   "metadata": {},
   "source": [
    "# üìñ Section 6: Evaluation and Metrics for LLMs\n",
    "\n",
    "Evaluating LLMs involves assessing how well they understand, reason, and generate language. This is crucial for ensuring models work correctly in real-world applications.\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- ‚úÖ Understand why LLM evaluation is important and challenging\n",
    "- ‚úÖ Learn common evaluation metrics (BLEU, ROUGE, perplexity, etc.)\n",
    "- ‚úÖ Explore human evaluation methods\n",
    "- ‚úÖ Understand task-specific evaluation approaches\n",
    "- ‚úÖ Practice evaluating LLM outputs\n",
    "\n",
    "## üìö What You'll Learn\n",
    "\n",
    "1. **Why Evaluate** - Importance of LLM evaluation\n",
    "2. **Automatic Metrics** - BLEU, ROUGE, perplexity, accuracy\n",
    "3. **Human Evaluation** - When and how to use human judges\n",
    "4. **Task-Specific Metrics** - Evaluation for different use cases\n",
    "5. **Evaluation Challenges** - Why evaluation is complex\n",
    "6. **Best Practices** - How to evaluate effectively"
   ]
  },
  {
   "cell_type": "code",
   "id": "90c35335-8110-43d8-a6b0-2adba7690765",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T03:01:20.184207Z",
     "start_time": "2025-12-16T03:01:19.551733Z"
    }
   },
   "source": [
    "# =============================\n",
    "# üìì SECTION 6: EVALUATION AND METRICS FOR LLMs\n",
    "# =============================\n",
    "\n",
    "%run ./utils_llm_connector.ipynb\n",
    "\n",
    "# Create a connector instance\n",
    "connector = LLMConnector()\n",
    "\n",
    "# Confirm connection\n",
    "print(\"üì° LLM Connector initialized and ready.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîë LLM Configuration Check:\n",
      "‚úÖ OpenAI API Details: FOUND\n",
      "‚úÖ Connected to OpenAI (model: gpt-4o)\n",
      "üì° LLM Connector initialized and ready.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "337e2d4f-a007-41d0-9d75-0bd04534391d",
   "metadata": {},
   "source": [
    "## üìä Why Evaluate LLMs?\n",
    "\n",
    "Evaluation is crucial because LLMs are deployed in critical applications where mistakes can have serious consequences.\n",
    "\n",
    "### Why It Matters\n",
    "\n",
    "LLMs are evaluated to ensure:\n",
    "- ‚úÖ **Quality of responses** - Accurate, relevant, and helpful outputs\n",
    "- ‚úÖ **Reliability and safety** - Consistent behavior, no harmful content\n",
    "- ‚úÖ **Domain-specific performance** - Works well for intended use cases\n",
    "- ‚úÖ **Fairness** - No bias against certain groups\n",
    "- ‚úÖ **Efficiency** - Performance meets latency and cost requirements\n",
    "\n",
    "### The Risks of Poor Evaluation\n",
    "\n",
    "Without proper evaluation, we risk:\n",
    "- üö® **Hallucinations**: Models generating plausible but false information\n",
    "- ‚ö†Ô∏è **Bias**: Discriminatory outputs against certain groups\n",
    "- üí• **Poor Performance**: Models failing in real-world scenarios\n",
    "- üîí **Safety Issues**: Generating harmful or dangerous content\n",
    "- üí∞ **Wasted Resources**: Deploying models that don't meet requirements\n",
    "\n",
    "### Real-World Impact\n",
    "\n",
    "- **Healthcare**: Incorrect medical advice could harm patients\n",
    "- **Legal**: Biased legal analysis could affect case outcomes\n",
    "- **Education**: Inaccurate information could mislead students\n",
    "- **Finance**: Wrong financial advice could cause losses"
   ]
  },
  {
   "cell_type": "code",
   "id": "f54bb5d0-5650-420f-a28f-5fbbc27da086",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T03:01:28.906947Z",
     "start_time": "2025-12-16T03:01:20.184910Z"
    }
   },
   "source": [
    "# Prompt: Explain why evaluating LLMs is important with 5 real-world analogies\n",
    "prompt = (\n",
    "    \"Explain why evaluating Large Language Models (LLMs) is important. \"\n",
    "    \"Provide 5 real-world analogies for better understanding.\"\n",
    ")\n",
    "\n",
    "response = connector.get_completion(prompt)\n",
    "print(response['content'] if isinstance(response, dict) else response)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content=\"Evaluating Large Language Models (LLMs) is crucial for several reasons, including ensuring their effectiveness, reliability, safety, and ethical alignment. Here are five real-world analogies to help illustrate the importance of evaluation:\\n\\n1. **Road Safety Testing for Vehicles**: Just like cars undergo rigorous safety testing before they hit the market to ensure they can be trusted on the road, LLMs need to be evaluated to ensure they produce accurate, relevant, and safe outputs before being deployed in applications that affect people's lives.\\n\\n2. **Quality Control in Food Production**: In food production, rigorous quality control is essential to ensure that products are safe to consume and meet consumer expectations. Similarly, evaluating LLMs ensures that the outputs are of high quality, free from harmful content, and meet the ethical standards expected by users.\\n\\n3. **Medical Trials for Pharmaceuticals**: New drugs undergo extensive trials to verify their efficacy and safety before they are approved for public use. In the same way, LLMs must be evaluated to ensure they perform as intended and do not produce harmful or misleading information.\\n\\n4. **Pilot Training and Simulation**: Pilots undergo extensive training and simulation exercises to ensure they can handle a wide range of scenarios safely. LLM evaluation involves testing the model's ability to handle diverse queries and scenarios to ensure it responds appropriately and accurately.\\n\\n5. **Software Testing in Development**: Software is thoroughly tested to identify bugs and improve performance before release. Evaluating LLMs is akin to this process, where the focus is on identifying biases, inaccuracies, and areas for improvement to ensure the model functions reliably in real-world applications.\\n\\nIn all these analogies, the underlying principle is that thorough evaluation is critical to ensure safety, reliability, and quality, thereby building trust and ensuring the responsible deployment of technology.\", refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "371266ce-8460-42ea-b208-2c3eed4bde61",
   "metadata": {},
   "source": [
    "## üìè Common Evaluation Metrics\n",
    "\n",
    "### üìù 1. Perplexity\n",
    "- Measures how well the model predicts the next word.\n",
    "- Lower is better.\n",
    "- üì¶ Analogy: Like a student's confidence in answering a quiz.\n",
    "\n",
    "### üìù 2. BLEU (Bilingual Evaluation Understudy)\n",
    "- Compares model output with reference text.\n",
    "- Higher means closer match.\n",
    "- üìñ Analogy: Like comparing student essays to a perfect answer key.\n",
    "\n",
    "### üìù 3. ROUGE (Recall-Oriented Understudy for Gisting Evaluation)\n",
    "- Measures overlap between generated and reference summaries.\n",
    "- üìÑ Analogy: Like checking how many key points a summary captures.\n",
    "\n",
    "### üìù 4. Accuracy\n",
    "- Used for classification tasks (e.g., sentiment analysis).\n",
    "- üéØ Analogy: Like measuring how often a dart hits the bullseye.\n",
    "\n",
    "### üìù 5. Human Evaluation\n",
    "- Judges assess quality based on fluency, relevance, and safety.\n",
    "- üë©‚Äç‚öñÔ∏è Analogy: Like food critics rating a chef‚Äôs dish."
   ]
  },
  {
   "cell_type": "code",
   "id": "371f1a9c-b499-4f1c-882b-d2ce89e130fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T03:01:54.863386Z",
     "start_time": "2025-12-16T03:01:28.949584Z"
    }
   },
   "source": [
    "# Hands-on Example: Evaluating LLM Outputs\n",
    "print(\"=\" * 60)\n",
    "print(\"üéØ Hands-on Example: Evaluation Metrics in Action\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Example: Summarization task\n",
    "reference_summary = \"AI is transforming industries through automation and intelligent decision-making.\"\n",
    "generated_summary = \"Artificial intelligence changes businesses by automating tasks and making smart choices.\"\n",
    "\n",
    "print(\"\\nüìä Example: Summarization Evaluation\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"Reference: {reference_summary}\")\n",
    "print(f\"Generated: {generated_summary}\")\n",
    "\n",
    "# Simple word overlap calculation (simplified ROUGE-1)\n",
    "ref_words = set(reference_summary.lower().split())\n",
    "gen_words = set(generated_summary.lower().split())\n",
    "overlap = len(ref_words & gen_words)\n",
    "total_ref = len(ref_words)\n",
    "recall = overlap / total_ref if total_ref > 0 else 0\n",
    "\n",
    "print(f\"\\nSimple Overlap Analysis:\")\n",
    "print(f\"  Reference words: {len(ref_words)}\")\n",
    "print(f\"  Generated words: {len(gen_words)}\")\n",
    "print(f\"  Overlapping words: {overlap}\")\n",
    "print(f\"  Approximate Recall: {recall:.2%}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üí° This is a simplified example - real metrics are more sophisticated!\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Ask LLM for detailed explanation\n",
    "prompt = (\n",
    "    \"List and explain 5 common evaluation metrics for Large Language Models. \"\n",
    "    \"Provide real-world analogies for each metric.\"\n",
    ")\n",
    "\n",
    "response = connector.get_completion(prompt)\n",
    "if hasattr(response, 'content'):\n",
    "    print(\"\\n\" + response.content)\n",
    "elif isinstance(response, dict):\n",
    "    print(\"\\n\" + response.get('content', str(response)))\n",
    "else:\n",
    "    print(\"\\n\" + str(response))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üéØ Hands-on Example: Evaluation Metrics in Action\n",
      "============================================================\n",
      "\n",
      "üìä Example: Summarization Evaluation\n",
      "------------------------------------------------------------\n",
      "Reference: AI is transforming industries through automation and intelligent decision-making.\n",
      "Generated: Artificial intelligence changes businesses by automating tasks and making smart choices.\n",
      "\n",
      "Simple Overlap Analysis:\n",
      "  Reference words: 9\n",
      "  Generated words: 11\n",
      "  Overlapping words: 1\n",
      "  Approximate Recall: 11.11%\n",
      "\n",
      "============================================================\n",
      "üí° This is a simplified example - real metrics are more sophisticated!\n",
      "============================================================\n",
      "\n",
      "Evaluating large language models (LLMs) involves a variety of metrics that help determine their performance across different tasks. Here are five common evaluation metrics, along with real-world analogies to help understand them:\n",
      "\n",
      "1. **Perplexity**:\n",
      "   - **Explanation**: Perplexity measures how well a language model predicts a sample. It's the exponential of the cross-entropy loss, where lower values indicate better performance. Essentially, it gauges the model's uncertainty when generating the next word in a sequence.\n",
      "   - **Analogy**: Imagine reading a book where you have to guess the next word in a sentence. A book with complex, unpredictable language would be like a story that confuses you often, resulting in high perplexity. Conversely, a straightforward narrative would have low perplexity, as you can easily anticipate what comes next.\n",
      "\n",
      "2. **BLEU (Bilingual Evaluation Understudy Score)**:\n",
      "   - **Explanation**: BLEU is primarily used for evaluating machine translation but can also apply to tasks like text summarization. It compares n-grams of the generated text with n-grams of reference texts, scoring translations based on their similarity to human translations.\n",
      "   - **Analogy**: Picture a student writing an essay based on a prompt. BLEU is akin to a teacher checking how many of the key phrases and ideas the student has correctly incorporated from the source material, rewarding those that closely match the original content.\n",
      "\n",
      "3. **ROUGE (Recall-Oriented Understudy for Gisting Evaluation)**:\n",
      "   - **Explanation**: ROUGE measures the overlap of n-grams between the generated text and reference texts, focusing on recall. It is often used in summarization tasks to evaluate how much relevant content from the reference is captured.\n",
      "   - **Analogy**: Think of a summary of a movie. ROUGE is like judging how well the summary covers all the important scenes and plot points compared to the actual movie, ensuring nothing critical is missed.\n",
      "\n",
      "4. **F1 Score**:\n",
      "   - **Explanation**: The F1 Score is the harmonic mean of precision and recall, providing a balance between the two. It is especially useful in classification tasks where one seeks a balance between false positives and false negatives.\n",
      "   - **Analogy**: Consider a sports referee making calls during a game. Precision would be making the right calls without false alarms, while recall would be catching every infraction. The F1 Score is like the referee's effectiveness at both accurately calling fouls and ensuring none go unnoticed.\n",
      "\n",
      "5. **Human Evaluation**:\n",
      "   - **Explanation**: Human evaluation involves subjective assessments by human judges, who rate the quality, coherence, and relevance of the generated text. It is often considered the gold standard in evaluating LLMs due to the nuanced understanding humans bring.\n",
      "   - **Analogy**: This is similar to a panel of judges in a cooking competition who taste and rate dishes based on flavor, presentation, and originality. Their combined expertise and subjective impressions determine the winner, much like human evaluators provide nuanced feedback on a model's outputs.\n",
      "\n",
      "These metrics together provide a comprehensive view of an LLM's performance, capturing both quantitative aspects and qualitative human judgments.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "5721d7be-bbf8-4866-983e-5b1921c40058",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è Challenges in Evaluation\n",
    "\n",
    "1. **Subjectivity**: What is ‚Äúgood‚Äù can vary across users.  \n",
    "2. **Context Sensitivity**: A response may be correct in one context but not another.  \n",
    "3. **Hallucinations**: Hard to detect automatically.  \n",
    "4. **Scalability**: Human evaluations don‚Äôt scale for billions of requests.  \n",
    "5. **Bias Detection**: Subtle biases are tricky to quantify.  \n",
    "\n",
    "### üìù Analogy\n",
    "Like grading creative writing essays‚Äîthere‚Äôs no single ‚Äúcorrect‚Äù answer."
   ]
  },
  {
   "cell_type": "code",
   "id": "a87c6736-7e91-4c62-9594-7ad90041ab06",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T03:02:04.322307Z",
     "start_time": "2025-12-16T03:01:54.870792Z"
    }
   },
   "source": [
    "# Prompt: List 5 challenges in evaluating LLMs with examples\n",
    "prompt = (\n",
    "    \"List 5 challenges in evaluating Large Language Models. \"\n",
    "    \"Provide a real-world analogy for each challenge.\"\n",
    ")\n",
    "\n",
    "response = connector.get_completion(prompt)\n",
    "print(response['content'] if isinstance(response, dict) else response)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content=\"Evaluating Large Language Models (LLMs) involves several challenges, each of which can be understood through real-world analogies:\\n\\n1. **Complexity in Understanding Output Quality:**\\n   - **Challenge:** It can be difficult to assess the quality and accuracy of the output generated by LLMs, as they often produce text that is syntactically correct but semantically incorrect or misleading.\\n   - **Analogy:** Like assessing a student's essay that is well-written but factually inaccurate, where the writing style is impressive, but the content is misleading or wrong.\\n\\n2. **Contextual Comprehension:**\\n   - **Challenge:** LLMs may struggle with understanding context or may not retain context over long conversations, leading to outputs that are irrelevant or inconsistent.\\n   - **Analogy:** Similar to a person who joins a conversation halfway through and starts making comments that don't fit the established context, creating confusion or disruption.\\n\\n3. **Bias and Ethical Concerns:**\\n   - **Challenge:** LLMs can inadvertently reflect or amplify biases present in the training data, leading to outputs that might be biased or ethically problematic.\\n   - **Analogy:** Comparable to a parrot that repeats phrases it has overheard without understanding their meaning or the potential impact, sometimes perpetuating harmful stereotypes.\\n\\n4. **Scalability of Human Evaluation:**\\n   - **Challenge:** Human evaluation of LLM outputs for quality, relevance, and coherence is time-consuming and may not be scalable given the vast amount of data produced by these models.\\n   - **Analogy:** Like trying to individually inspect every product on an assembly line for defects when the production speed is incredibly high, making it nearly impossible to evaluate every item thoroughly.\\n\\n5. **Dynamically Changing Language and Knowledge:**\\n   - **Challenge:** LLMs may not keep up with the latest information or changes in language usage, as they are trained on data up to a certain point and may not be updated frequently.\\n   - **Analogy:** Similar to an encyclopedia that is printed once and never updated, where the information becomes outdated over time, missing new developments and changes in common knowledge.\\n\\nThese challenges highlight the complexity of working with LLMs and the need for ongoing research and development to improve their evaluation and usability.\", refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "293af0a3-c4bb-4ead-8343-f504bbca17aa",
   "metadata": {},
   "source": [
    "## üìù Example: Simple Perplexity Approximation\n",
    "\n",
    "While true perplexity requires access to model internals, you can approximate it by asking the LLM to predict the next word in incomplete sentences and measuring its confidence."
   ]
  },
  {
   "cell_type": "code",
   "id": "7297a4b5-b230-4b6f-927a-d3b17d3d7975",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T03:02:05.923754Z",
     "start_time": "2025-12-16T03:02:04.331667Z"
    }
   },
   "source": [
    "# Example: Asking model to predict next word\n",
    "prompt = (\n",
    "    \"Complete the following sentence and explain your confidence: \"\n",
    "    \"'The capital of France is'\"\n",
    ")\n",
    "\n",
    "response = connector.get_completion(prompt)\n",
    "print(response['content'] if isinstance(response, dict) else response)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content='The capital of France is Paris.\\n\\nI am highly confident in this response because Paris has been the capital city of France for many centuries, and this is a well-documented and widely known fact in geography and world history. Paris is not only the political center of France but also a major cultural and economic hub, making it a prominent city that is frequently referenced in various contexts.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "b5d57e0a-79a4-4fa5-92d0-a64a95402b1f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Evaluation Best Practices\n",
    "\n",
    "### 1. Use Multiple Metrics\n",
    "- Don't rely on a single metric\n",
    "- Combine automatic and human evaluation\n",
    "- **Example**: Use BLEU + human ratings for translation\n",
    "\n",
    "### 2. Task-Specific Evaluation\n",
    "- Choose metrics appropriate for your task\n",
    "- **Example**: Use Pass@k for code generation, BLEU for translation\n",
    "\n",
    "### 3. Diverse Test Sets\n",
    "- Include various examples, edge cases, and demographics\n",
    "- **Example**: Test on different languages, domains, difficulty levels\n",
    "\n",
    "### 4. Continuous Evaluation\n",
    "- Monitor model performance in production\n",
    "- Track metrics over time\n",
    "- **Example**: A/B testing different model versions\n",
    "\n",
    "### 5. Human-in-the-Loop\n",
    "- Use human evaluation for critical decisions\n",
    "- Combine with automatic metrics\n",
    "- **Example**: Human review for medical advice generation\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Summary\n",
    "\n",
    "In this notebook, we've covered:\n",
    "\n",
    "‚úÖ **Why Evaluate** - Importance of LLM evaluation for quality and safety  \n",
    "‚úÖ **Automatic Metrics** - BLEU, ROUGE, perplexity, accuracy, F1 score  \n",
    "‚úÖ **Human Evaluation** - When and how to use human judges  \n",
    "‚úÖ **Task-Specific Metrics** - Evaluation approaches for different use cases  \n",
    "‚úÖ **Challenges** - Subjectivity, hallucinations, scalability, bias  \n",
    "‚úÖ **Best Practices** - How to evaluate effectively  \n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **Evaluation is essential** for safe and effective LLM deployment\n",
    "- **No single metric** captures all aspects of quality\n",
    "- **Combine automatic and human** evaluation for best results\n",
    "- **Task-specific metrics** are crucial for accurate assessment\n",
    "- **Continuous evaluation** ensures models maintain quality over time\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **Notebook 7**: Learn about ethical considerations and bias\n",
    "- **Notebook 8**: Understand fine-tuning and its evaluation\n",
    "- **Practice**: Evaluate outputs from different prompts\n",
    "\n",
    "---\n",
    "\n",
    "## üéì Try It Yourself!\n",
    "\n",
    "**Exercise 1**: Generate two summaries of the same text using different prompts. Compare them using simple word overlap.\n",
    "\n",
    "**Exercise 2**: Ask an LLM the same question multiple times. Evaluate consistency of responses.\n",
    "\n",
    "**Exercise 3**: Create a simple evaluation rubric for a specific task (e.g., email generation).\n",
    "\n",
    "**Exercise 4**: Research evaluation benchmarks like GLUE, SuperGLUE, or HELM. What do they measure?  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
