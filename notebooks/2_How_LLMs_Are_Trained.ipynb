{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de404e36-8ec8-4499-a7e3-4197598d867c",
   "metadata": {},
   "source": [
    "# üìñ Section 2: How Large Language Models Are Trained\n",
    "\n",
    "Large Language Models (LLMs) are built through complex training processes that turn raw text data into powerful predictive systems.  \n",
    "\n",
    "In this section, we‚Äôll explore:  \n",
    "‚úÖ The two stages of training: **Pre-training** and **Fine-tuning**  \n",
    "‚úÖ Data requirements and challenges  \n",
    "‚úÖ How LLMs learn to generate and understand language  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a14f3287-d5ad-4a84-8c31-de4675f00cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîë LLM Configuration Check:\n",
      "‚úÖ Azure API Details: FOUND\n",
      "‚úÖ Connected to Azure OpenAI (deployment: gpt-4o)\n",
      "üì° LLM Connector initialized and ready.\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# üìì SECTION 2: HOW LLMs ARE TRAINED\n",
    "# =============================\n",
    "\n",
    "%run ./utils_llm_connector.ipynb\n",
    "\n",
    "# Create a connector instance\n",
    "connector = LLMConnector()\n",
    "\n",
    "# Confirm connection\n",
    "print(\"üì° LLM Connector initialized and ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ac04de-f37e-4023-a2ed-9add2647d19c",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Training Process Overview\n",
    "\n",
    "LLMs are trained in two main stages:\n",
    "\n",
    "1. **Pre-training**  \n",
    "   - The model learns from massive amounts of text (books, articles, code, etc.).  \n",
    "   - Goal: Predict the next word in a sequence (unsupervised learning).\n",
    "\n",
    "2. **Fine-tuning**  \n",
    "   - The model is refined on smaller, domain-specific datasets.  \n",
    "   - Goal: Specialize in tasks like coding, legal writing, or healthcare conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1c8daba-fd1d-492e-98ff-eaa330dbb410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content='Certainly! Let\\'s break down the difference between pre-training and fine-tuning of large language models using analogies and examples that are easy to understand.\\n\\n---\\n\\n### **Pre-training: The Foundation**\\nThink of pre-training as the broad and general education phase where a model learns basic knowledge about the world and language. It‚Äôs like going to school to learn reading, writing, math, history, and science. During this stage, the model is exposed to a vast amount of text data and learns patterns, grammar, relationships between words, and general knowledge.\\n\\n#### **Analogy for Pre-training:**\\nImagine teaching a child to read and write by exposing them to thousands of books. These books cover every topic imaginable‚Äîscience, literature, history, cooking, sports, etc. The child doesn‚Äôt specialize in anything yet but builds a solid foundation of knowledge.\\n\\n#### **Examples of Pre-training:**\\n1. **Reading a library full of books**: The model \"reads\" a massive dataset that includes diverse content like Wikipedia articles, news stories, novels, and technical documents. It learns how language works and gathers general knowledge.\\n2. **Learning to recognize patterns**: Just as a child learns that \"red + blue = purple\" or \"cats are animals,\" the model learns associations and patterns between words and concepts.\\n3. **Building vocabulary and grammar skills**: Similar to how a student learns grammar rules, spelling, and sentence structure, the model learns how sentences are constructed and how words interact.\\n\\n---\\n\\n### **Fine-tuning: The Specialization**\\nFine-tuning is like giving the model a focused training session tailored to a specific task or industry. It‚Äôs like taking someone who has general knowledge and training them to excel in a specific job, like being a chef, lawyer, or engineer.\\n\\n#### **Analogy for Fine-tuning:**\\nImagine a chef who has general cooking skills and knows how to prepare all kinds of meals. Now, they are trained specifically in French cuisine. They tweak their broad cooking knowledge to master French dishes like souffl√© or coq au vin.\\n\\n#### **Examples of Fine-tuning:**\\n1. **Training a customer service chatbot**: A general language model is fine-tuned using conversation data from a company. It learns to respond to questions about the company‚Äôs products, policies, and services.\\n2. **Medical diagnosis assistant**: A model is fine-tuned with medical research papers and patient data so it can help doctors analyze symptoms and suggest possible diagnoses.\\n3. **Legal document analysis**: A model is fine-tuned with legal cases and contracts to assist lawyers in reviewing documents or finding relevant precedents.\\n\\n---\\n\\n### **Key Difference Recap:**\\n- **Pre-training** gives the model broad, general knowledge about language and the world.\\n- **Fine-tuning** tailors the model to perform well in specific tasks or domains.\\n\\nIn simpler terms:\\n- Pre-training is like teaching someone everything about how to read and understand the world.\\n- Fine-tuning is like teaching them how to apply that knowledge to be really good at one specific thing.\\n\\n', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "# Prompt: Explain pre-training and fine-tuning with analogies\n",
    "prompt = (\n",
    "    \"Explain the difference between pre-training and fine-tuning of Large Language Models. \"\n",
    "    \"Use analogies a non-technical person can relate to and give 3 examples for each.\"\n",
    ")\n",
    "\n",
    "response = connector.get_completion(prompt)\n",
    "print(response['content'] if isinstance(response, dict) else response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09812281-bf41-4dc8-b975-1904ded8b88d",
   "metadata": {},
   "source": [
    "## üìö Pre-training in Detail\n",
    "\n",
    "During pre-training, the LLM learns from vast datasets using a process called **next-word prediction**.  \n",
    "\n",
    "### üìù Example Analogies\n",
    "- üß† Like reading the entire Wikipedia and trying to guess the next sentence.  \n",
    "- üé® Like an artist sketching millions of scenes to understand patterns.  \n",
    "- üéπ Like a pianist memorizing thousands of songs before improvising.  \n",
    "- üèãÔ∏è‚Äç‚ôÇÔ∏è Like a bodybuilder lifting weights to build general strength.  \n",
    "- üõ†Ô∏è Like a mechanic studying every car manual before working on real vehicles.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10dbcb01-50a0-4127-84f0-16ea6af0b906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content='Absolutely! Here are five real-world analogies to help explain the concept of pre-training in Large Language Models (LLMs):\\n\\n---\\n\\n### 1. **Learning Basic Skills Before a Job**\\nImagine you\\'re training to be a chef. Before working in a restaurant, you first learn general cooking techniques‚Äîhow to chop vegetables, bake bread, and season dishes. This foundational knowledge prepares you to handle specific recipes when you\\'re on the job.  \\n**Pre-training in LLMs** works similarly: the model first \"learns\" general language patterns and structures by analyzing vast amounts of text before being fine-tuned for specific tasks (like answering questions or generating code).\\n\\n---\\n\\n### 2. **Reading a Dictionary Before Writing a Book**\\nSuppose you want to write a novel. Before you begin, you spend weeks reading dictionaries, encyclopedias, and books to understand words, grammar, and concepts. Once you\\'ve absorbed this knowledge, you\\'re better equipped to string together meaningful sentences and stories.  \\nIn LLMs, **pre-training** is like this preparatory phase, where the model learns the \"language rules\" by processing enormous text datasets.\\n\\n---\\n\\n### 3. **Practicing Piano Scales Before Playing Songs**\\nA pianist practices scales and arpeggios repeatedly to develop muscle memory and understand the structure of music. Once these foundational skills are mastered, they can confidently play complex compositions.  \\n**Pre-training** is analogous to this practice‚Äîit teaches the model general linguistic patterns, enabling it to \"play\" specific tasks later.\\n\\n---\\n\\n### 4. **Learning to Swim Before Competing**\\nBefore entering a swimming competition, you first learn how to float, kick, and stroke. These general skills are essential for any swimming event, even if the competition itself focuses on specific techniques like butterfly or freestyle.  \\nSimilarly, **pre-training** equips LLMs with the \"basic swimming skills\" of language, preparing them for specialized tasks during fine-tuning.\\n\\n---\\n\\n### 5. **Building a Toolkit Before Solving Problems**\\nImagine assembling a toolbox with hammers, screwdrivers, and wrenches. You might not know exactly what problem you\\'ll face later, but having a well-stocked toolkit means you\\'re ready for any challenge that arises‚Äîwhether it\\'s fixing a bike or assembling furniture.  \\n**Pre-training** gives LLMs a \"linguistic toolkit,\" allowing them to handle diverse tasks effectively when specific instructions are provided.\\n\\n---\\n\\nEach analogy highlights how pre-training involves learning broad, foundational knowledge that prepares LLMs to tackle specific tasks with greater efficiency!', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "# Prompt: Provide 5 real-world analogies for LLM pre-training\n",
    "prompt = (\n",
    "    \"Give 5 real-world analogies to explain pre-training in Large Language Models. \"\n",
    "    \"Each analogy should be relatable and simple.\"\n",
    ")\n",
    "\n",
    "response = connector.get_completion(prompt)\n",
    "print(response['content'] if isinstance(response, dict) else response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eef954d-0010-4835-9d0b-d135e922baee",
   "metadata": {},
   "source": [
    "## üéØ Fine-tuning in Detail\n",
    "\n",
    "Fine-tuning specializes the LLM on focused tasks after general pre-training.\n",
    "\n",
    "### üìù Example Analogies\n",
    "- üë®‚Äçüç≥ A chef specializing in French cuisine after learning all world cuisines.  \n",
    "- üèÉ‚Äç‚ôÄÔ∏è An athlete training specifically for marathons after general fitness.  \n",
    "- üìö A student studying law after general education.  \n",
    "- üéπ A pianist focusing on jazz after classical training.  \n",
    "- üõ†Ô∏è A mechanic specializing in electric vehicles.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64566052-4663-41ef-90c5-c454d203fa81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content='Here are five real-world analogies to explain fine-tuning in large language models, emphasizing the concept of specialization after general training:\\n\\n---\\n\\n### 1. **University Education to Career Training**\\n- **General Training:** A student completes their undergraduate degree, studying a broad range of subjects (e.g., math, science, literature, etc.). This builds a foundational understanding of various disciplines.\\n- **Fine-Tuning:** After graduation, the student enrolls in a specialized program or receives on-the-job training to become an expert in a specific field, like medicine, law, or software engineering. The general education serves as the base, while the career training hones their skills for a particular profession.\\n\\n---\\n\\n### 2. **Cooking Basics to Cuisine Specialization**\\n- **General Training:** A chef learns basic cooking techniques, such as chopping, frying, baking, and seasoning, without focusing on any specific cuisine.\\n- **Fine-Tuning:** The chef then trains in a specialized cooking style, like French patisserie, Japanese sushi-making, or Italian pasta dishes, mastering recipes and techniques unique to that cuisine. The foundational skills enable them to excel in their niche.\\n\\n---\\n\\n### 3. **Gym Fitness to Sport-Specific Coaching**\\n- **General Training:** A person works out at the gym, focusing on general fitness‚Äîbuilding strength, endurance, flexibility, and coordination.\\n- **Fine-Tuning:** They later train specifically for a sport like tennis, basketball, or swimming, focusing on specialized movements, strategies, and techniques required for that activity. Their general fitness lays the groundwork for sport-specific expertise.\\n\\n---\\n\\n### 4. **Language Learning to Professional Translation**\\n- **General Training:** A student learns a new language, acquiring vocabulary, grammar, and conversational skills for general communication.\\n- **Fine-Tuning:** The student then trains as a professional translator, specializing in technical, legal, or medical texts. This requires adapting their general language knowledge to specific terminology, nuances, and context.\\n\\n---\\n\\n### 5. **Basic Carpentry to Furniture Making**\\n- **General Training:** A carpenter learns general woodworking skills, such as measuring, cutting, sanding, and assembling materials.\\n- **Fine-Tuning:** The carpenter specializes in crafting high-end furniture, mastering intricate designs and techniques like carving or veneering. Their general carpentry skills enable them to refine their craft for specialized creations.\\n\\n---\\n\\nEach analogy highlights how a broad foundation (general training) is followed by targeted refinement (fine-tuning) to achieve expertise in a specific domain, similar to how large language models are fine-tuned for specialized tasks after general pretraining.', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "# Prompt: Provide 5 real-world analogies for LLM fine-tuning\n",
    "prompt = (\n",
    "    \"Give 5 real-world analogies to explain fine-tuning in Large Language Models. \"\n",
    "    \"Each analogy should highlight specialization after general training.\"\n",
    ")\n",
    "\n",
    "response = connector.get_completion(prompt)\n",
    "print(response['content'] if isinstance(response, dict) else response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397a09c2-4095-4041-a890-de02593e3b95",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è Challenges in Training LLMs\n",
    "\n",
    "Training LLMs is not trivial. Major challenges include:  \n",
    "\n",
    "1. üì¶ **Data Quality**: Avoiding biased or harmful content.  \n",
    "2. üí∞ **Compute Resources**: High costs for GPUs and infrastructure.  \n",
    "3. üîÑ **Continual Learning**: Adapting models to new data.  \n",
    "4. üåç **Language Diversity**: Supporting multiple languages and dialects.  \n",
    "5. üîê **Privacy Concerns**: Ensuring sensitive data isn‚Äôt leaked.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a11cfd1-ec75-499c-8658-9426078e5103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content=\"Training large language models (LLMs) like GPT involves numerous technical, ethical, and practical challenges. Below are five major challenges, along with explanations and real-world examples for each:\\n\\n---\\n\\n### 1. **Data Quality and Bias**\\n#### Explanation:\\nLLMs rely heavily on massive datasets to learn patterns in language. If the training data contains biased, incomplete, or incorrect information, the model may perpetuate those biases or inaccuracies. Ensuring diverse, high-quality, and representative data is critical but difficult to achieve.\\n\\n#### Real-World Example:\\nOpenAI's GPT models have faced criticism for occasionally producing biased or stereotypical responses. For example, earlier versions of GPT were found to generate text that reflected gender or racial biases present in the internet data they were trained on. Efforts to mitigate bias, such as filtering training datasets, are ongoing but imperfect.\\n\\n---\\n\\n### 2. **Computational Costs**\\n#### Explanation:\\nTraining LLMs requires enormous amounts of computational power, memory, and energy. This makes the process expensive and environmentally taxing, as it consumes significant electricity and hardware resources. Scaling models further exacerbates these costs.\\n\\n#### Real-World Example:\\nGoogle's DeepMind spent millions of dollars in computational resources to train models like AlphaCode and Gopher. Similarly, OpenAI's GPT-3 reportedly cost several million dollars to train, requiring hundreds of high-performance GPUs over weeks or months.\\n\\n---\\n\\n### 3. **Overfitting and Generalization**\\n#### Explanation:\\nLLMs risk overfitting to their training data, meaning they perform well on known inputs but struggle to generalize to unseen contexts. This is problematic when models are expected to handle diverse real-world tasks without specific tuning.\\n\\n#### Real-World Example:\\nAn LLM trained primarily on English-language internet data may struggle with understanding nuanced cultural or linguistic contexts in less-represented languages like Swahili or indigenous languages. This can result in poor performance or irrelevant outputs when applied to such scenarios.\\n\\n---\\n\\n### 4. **Ethical and Safety Concerns**\\n#### Explanation:\\nLLMs can generate harmful, misleading, or offensive content, which raises ethical concerns. Additionally, adversaries can exploit LLMs for malicious purposes, like generating fake news, phishing emails, or propaganda.\\n\\n#### Real-World Example:\\nThe GPT-3 model has been used to generate realistic fake news articles that could manipulate public opinion if disseminated widely. For instance, researchers demonstrated how GPT-3 could produce plausible misinformation on topics like climate change or health.\\n\\n---\\n\\n### 5. **Scalability and Fine-Tuning**\\n#### Explanation:\\nScaling LLMs to larger sizes increases their capacity to understand complex patterns but also makes fine-tuning on specific tasks more challenging. The larger the model, the harder it is to adapt it efficiently to new domains or applications without retraining.\\n\\n#### Real-World Example:\\nIndustries like healthcare require LLMs to understand domain-specific terminology and context. Fine-tuning GPT models for medical applications, such as symptom diagnosis or drug interactions, is challenging because it requires specialized datasets and careful calibration to avoid harmful recommendations.\\n\\n---\\n\\n### Conclusion:\\nThe challenges of training LLMs span technical, ethical, and practical domains, requiring interdisciplinary solutions from AI researchers, ethicists, and policymakers. Real-world examples highlight the importance of addressing these challenges to ensure LLMs are safe, reliable, and equitable for widespread use.\", refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "# Prompt: List 5 major challenges in training LLMs with real-world examples\n",
    "prompt = (\n",
    "    \"List and explain 5 major challenges in training Large Language Models (LLMs). \"\n",
    "    \"Give a real-world example for each challenge.\"\n",
    ")\n",
    "\n",
    "response = connector.get_completion(prompt)\n",
    "print(response['content'] if isinstance(response, dict) else response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b77afc5-873a-4ff3-a0d4-6e96ea35c38e",
   "metadata": {},
   "source": [
    "## ‚úÖ Summary\n",
    "\n",
    "In this section, we covered:  \n",
    "- The two main stages of LLM training: pre-training and fine-tuning.  \n",
    "- Real-world analogies to simplify complex concepts.  \n",
    "- Key challenges faced during LLM training.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
