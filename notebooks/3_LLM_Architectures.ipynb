{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7da79261-8d8f-47c7-b36e-8260f5a2f7b9",
   "metadata": {},
   "source": [
    "# üìñ Section 3: LLM Architectures\n",
    "\n",
    "At the heart of every Large Language Model (LLM) lies an architecture designed for understanding and generating language.  \n",
    "\n",
    "This section explores:  \n",
    "‚úÖ Transformers and the ‚ÄúAttention‚Äù mechanism  \n",
    "‚úÖ Key architectural components (Embedding Layers, Encoder-Decoder, etc.)  \n",
    "‚úÖ How these enable LLMs to process vast amounts of text efficiently  "
   ]
  },
  {
   "cell_type": "code",
   "id": "61465554-b8fa-429f-aa2d-934010df0174",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T03:01:11.028446Z",
     "start_time": "2025-12-16T03:01:10.334612Z"
    }
   },
   "source": [
    "# =============================\n",
    "# üìì SECTION 3: LLM ARCHITECTURES\n",
    "# =============================\n",
    "\n",
    "%run ./utils_llm_connector.ipynb\n",
    "\n",
    "# Create a connector instance\n",
    "connector = LLMConnector()\n",
    "\n",
    "# Confirm connection\n",
    "print(\"üì° LLM Connector initialized and ready.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîë LLM Configuration Check:\n",
      "‚úÖ OpenAI API Details: FOUND\n",
      "‚úÖ Connected to OpenAI (model: gpt-4o)\n",
      "üì° LLM Connector initialized and ready.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "ec9f436a-db73-4633-90a0-d0d6802aa50c",
   "metadata": {},
   "source": [
    "## üî• Transformers: The Backbone of LLMs\n",
    "\n",
    "Transformers revolutionized NLP by introducing a mechanism called **‚ÄúAttention‚Äù**, allowing models to focus on different parts of input text dynamically.  \n",
    "\n",
    "They process words **in parallel** instead of sequentially, making them fast and scalable.  \n",
    "\n",
    "### üìù Example Analogies\n",
    "- üéØ **Spotlight at a Concert**: Focuses on different performers depending on the song.  \n",
    "- üë©‚Äçüè´ **Teacher Highlighting Key Text**: Emphasizes important words in a paragraph.  \n",
    "- üß≠ **Navigator**: Pays more attention to landmarks when giving directions.  \n",
    "- üì∞ **Editor Scanning an Article**: Zeroes in on relevant sections for a summary.  \n",
    "- üõ†Ô∏è **Multi-tool**: Adapts to whatever task is needed in real time.  "
   ]
  },
  {
   "cell_type": "code",
   "id": "08265c6e-57b0-4c50-a16f-87038e067653",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T03:01:26.498938Z",
     "start_time": "2025-12-16T03:01:11.029225Z"
    }
   },
   "source": [
    "# Hands-on Example: Understanding Attention\n",
    "print(\"=\" * 60)\n",
    "print(\"üéØ Hands-on Example: How Attention Works\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Demonstrate attention with a simple example\n",
    "sentence = \"The cat sat on the mat\"\n",
    "words = sentence.split()\n",
    "\n",
    "print(f\"\\nSentence: '{sentence}'\")\n",
    "print(\"\\nWhen processing each word, attention focuses on relevant words:\")\n",
    "print(\"\\nWord: 'cat'\")\n",
    "print(\"  High attention: 'The' (article), 'sat' (action)\")\n",
    "print(\"  Medium attention: 'on', 'the', 'mat' (context)\")\n",
    "\n",
    "print(\"\\nWord: 'mat'\")\n",
    "print(\"  High attention: 'cat' (subject), 'sat' (action), 'on' (preposition)\")\n",
    "print(\"  Medium attention: 'The', 'the' (articles)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üí° Attention allows the model to understand relationships!\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Ask LLM to explain\n",
    "prompt = (\n",
    "    \"Explain how transformers and the attention mechanism work in Large Language Models. \"\n",
    "    \"Provide 5 real-world analogies to make it simple for non-technical readers.\"\n",
    ")\n",
    "\n",
    "response = connector.get_completion(prompt)\n",
    "if hasattr(response, 'content'):\n",
    "    print(\"\\n\" + response.content)\n",
    "elif isinstance(response, dict):\n",
    "    print(\"\\n\" + response.get('content', str(response)))\n",
    "else:\n",
    "    print(\"\\n\" + str(response))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üéØ Hands-on Example: How Attention Works\n",
      "============================================================\n",
      "\n",
      "Sentence: 'The cat sat on the mat'\n",
      "\n",
      "When processing each word, attention focuses on relevant words:\n",
      "\n",
      "Word: 'cat'\n",
      "  High attention: 'The' (article), 'sat' (action)\n",
      "  Medium attention: 'on', 'the', 'mat' (context)\n",
      "\n",
      "Word: 'mat'\n",
      "  High attention: 'cat' (subject), 'sat' (action), 'on' (preposition)\n",
      "  Medium attention: 'The', 'the' (articles)\n",
      "\n",
      "============================================================\n",
      "üí° Attention allows the model to understand relationships!\n",
      "============================================================\n",
      "\n",
      "Transformers and the attention mechanism are key components of large language models (LLMs) like GPT, BERT, and others. They enable these models to understand and generate human language with impressive accuracy. Here's a simplified explanation, followed by some real-world analogies to help make it more relatable:\n",
      "\n",
      "### Explanation of Transformers and Attention Mechanism\n",
      "\n",
      "1. **Transformers**: At a high level, a transformer is a type of neural network architecture designed to process and generate sequences of data, such as sentences or paragraphs. Unlike previous models, transformers handle entire sentences or paragraphs at once, rather than processing them word by word or character by character.\n",
      "\n",
      "2. **Attention Mechanism**: Within transformers, the attention mechanism allows the model to focus on specific parts of the input data when making predictions. It assigns different levels of importance, or \"attention,\" to each part of the input, helping the model determine which words or phrases are most relevant in understanding the context or generating a response.\n",
      "\n",
      "### Real-World Analogies\n",
      "\n",
      "1. **Reading Glasses**:\n",
      "   - **Transformer**: Imagine reading a book where the text is blurry, but you have special reading glasses that let you focus on the important words to understand the story. The transformer is like these glasses, helping you focus on the right parts of the text all at once.\n",
      "   - **Attention**: The attention mechanism is like adjusting the lens to focus on certain words more than others, ensuring you catch the main ideas and nuances.\n",
      "\n",
      "2. **Chef with Ingredients**:\n",
      "   - **Transformer**: Picture a chef preparing a dish. They have all the ingredients laid out in front of them. The transformer is like the chef, considering all ingredients at once to create a balanced dish.\n",
      "   - **Attention**: The attention mechanism is akin to the chef prioritizing certain ingredients over others, like adding more salt to enhance flavor, based on the overall taste they want to achieve.\n",
      "\n",
      "3. **Group Conversation**:\n",
      "   - **Transformer**: Imagine sitting in a group discussion. A transformer is like having the ability to listen to everyone at the same time, understanding the flow of conversation as a whole.\n",
      "   - **Attention**: Attention in this scenario is like selectively tuning into specific speakers whose comments are most relevant to the topic being discussed, ensuring you get the key points.\n",
      "\n",
      "4. **Tour Guide**:\n",
      "   - **Transformer**: Think of a tour guide showing visitors around a museum. The transformer acts like the guide, presenting all exhibits at once to give a complete picture of the museum's offerings.\n",
      "   - **Attention**: The attention mechanism is like the guide highlighting certain exhibits that are most significant or interesting, ensuring visitors focus on the highlights of the tour.\n",
      "\n",
      "5. **Symphony Conductor**:\n",
      "   - **Transformer**: Consider a conductor leading an orchestra. The transformer is like the conductor, overseeing all musicians to create a harmonious performance.\n",
      "   - **Attention**: Attention is akin to the conductor emphasizing specific instruments at certain times to enhance the overall sound, much like focusing on different parts of a sentence to capture meaning.\n",
      "\n",
      "These analogies aim to convey how transformers and attention mechanisms work together to process and generate language effectively, by focusing on the most relevant parts of the input data to capture nuances and context.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "a70fead1-753f-459a-8e81-baab1b43ba7f",
   "metadata": {},
   "source": [
    "## üß± Key Components of Transformer Architecture\n",
    "\n",
    "Transformers consist of several core components working together:\n",
    "\n",
    "### 1. **Embedding Layer**\n",
    "- **Purpose**: Converts words/tokens into numerical vectors (dense representations)\n",
    "- **Why**: Neural networks work with numbers, not text\n",
    "- **Example**: \"cat\" ‚Üí [0.2, -0.5, 0.8, ..., 0.1] (vector of 768 or more dimensions)\n",
    "- **Analogy**: Like assigning unique barcodes to every word, but the barcodes capture meaning\n",
    "\n",
    "### 2. **Positional Encoding**\n",
    "- **Purpose**: Adds information about word position/order in the sequence\n",
    "- **Why**: Transformers process words in parallel, so they need position info\n",
    "- **Example**: \"cat sat\" vs \"sat cat\" - different meanings, same words\n",
    "- **Analogy**: Like adding musical notes to indicate timing in a melody\n",
    "\n",
    "### 3. **Multi-head Self-Attention**\n",
    "- **Purpose**: Allows the model to attend to multiple parts of input simultaneously\n",
    "- **Why**: Different \"heads\" can focus on different types of relationships\n",
    "- **Example**: One head focuses on syntax, another on semantics\n",
    "- **Analogy**: Like watching a movie from multiple camera angles at once\n",
    "\n",
    "### 4. **Feed-Forward Neural Networks**\n",
    "- **Purpose**: Processes and transforms information after attention\n",
    "- **Why**: Adds non-linearity and learns complex transformations\n",
    "- **Example**: Combines information from attention to form new representations\n",
    "- **Analogy**: Processing ingredients in a recipe step by step\n",
    "\n",
    "### 5. **Layer Normalization & Residual Connections**\n",
    "- **Purpose**: Stabilizes training and allows deeper networks\n",
    "- **Why**: Helps gradients flow and prevents training issues\n",
    "- **Example**: Normalizes activations, adds skip connections\n",
    "- **Analogy**: Like safety nets and shortcuts in a complex building\n",
    "\n",
    "### Architecture Variants\n",
    "\n",
    "**Decoder-Only (GPT-style)**\n",
    "- Uses only decoder layers\n",
    "- Good for: Text generation, completion\n",
    "- Examples: GPT-3, GPT-4, ChatGPT\n",
    "\n",
    "**Encoder-Only (BERT-style)**\n",
    "- Uses only encoder layers\n",
    "- Good for: Understanding, classification\n",
    "- Examples: BERT, RoBERTa\n",
    "\n",
    "**Encoder-Decoder (T5-style)**\n",
    "- Uses both encoder and decoder\n",
    "- Good for: Translation, summarization\n",
    "- Examples: T5, BART\n",
    "\n",
    "### üìù Example Analogies\n",
    "- üì¶ **Embedding**: Like assigning unique barcodes to every word.  \n",
    "- üéº **Positional Encoding**: Adding musical notes to indicate timing in a melody.  \n",
    "- üëÄ **Multi-head Attention**: Like watching a movie from multiple camera angles at once.  \n",
    "- üç≥ **Feed-Forward Layers**: Processing ingredients in a recipe step by step.  \n",
    "- üèóÔ∏è **Encoder-Decoder**: Architect designing a blueprint, then workers building it."
   ]
  },
  {
   "cell_type": "code",
   "id": "8d75a606-0e24-4a54-8a13-48306dbc6776",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T03:01:35.854004Z",
     "start_time": "2025-12-16T03:01:26.548209Z"
    }
   },
   "source": [
    "# Prompt: List and explain key components of transformer architecture with analogies\n",
    "prompt = (\n",
    "    \"List and explain the key components of transformer architecture used in Large Language Models. \"\n",
    "    \"Provide 5 real-world analogies to make each concept relatable.\"\n",
    ")\n",
    "\n",
    "response = connector.get_completion(prompt)\n",
    "print(response['content'] if isinstance(response, dict) else response)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content=\"Transformer architecture is a pivotal advancement in the development of Large Language Models (LLMs). Here are the key components of transformer architecture, each paired with real-world analogies to make them more relatable:\\n\\n1. **Self-Attention Mechanism**:\\n   - **Explanation**: This mechanism allows the model to weigh the importance of different words in a sentence relative to each other. It provides context by considering the relationship between words, irrespective of their position.\\n   - **Analogy**: Imagine reading a mystery novel where every detail could be a clue. Self-attention is like having a mental highlighter that dynamically emphasizes important clues as you gather more context through the story.\\n\\n2. **Multi-Head Attention**:\\n   - **Explanation**: This extends the self-attention mechanism by having multiple attention heads, each learning different aspects of the sentence. It allows the model to focus on various parts of the sentence simultaneously.\\n   - **Analogy**: Think of a group of detectives working on a case. Each detective looks at the same evidence but focuses on different aspects, such as motives, alibis, or physical evidence, to piece together the full story.\\n\\n3. **Feed-Forward Neural Networks**:\\n   - **Explanation**: After the attention mechanism, the data passes through a feed-forward neural network. This network processes inputs independently for each position and adds non-linear transformations.\\n   - **Analogy**: Consider a chef who, after selecting ingredients (attention), uses a recipe (feed-forward network) to transform those raw ingredients into a finished dish, adding flavors and textures that were not apparent in the individual components.\\n\\n4. **Positional Encoding**:\\n   - **Explanation**: Since transformer models do not inherently understand the order of words, positional encoding is added to provide information about the position of words in a sentence.\\n   - **Analogy**: Think of a music conductor who, despite hearing all the instruments at once, knows exactly when each instrument should play by looking at the conductor's score, ensuring that the music is performed in the correct sequence.\\n\\n5. **Layer Normalization**:\\n   - **Explanation**: This component stabilizes the learning process by normalizing the inputs to each layer, ensuring that the model's learning dynamics are consistent and efficient.\\n   - **Analogy**: Imagine a basketball coach who ensures each player is in peak physical condition before every game. By consistently maintaining physical standards, the team performs optimally regardless of the game's challenges.\\n\\nThese components work together to create a powerful architecture that can understand and generate human-like text, making transformers the backbone of many modern LLMs.\", refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "735c51e0-5438-40f3-9e15-e1129773e1a8",
   "metadata": {},
   "source": [
    "## üöÄ Why Transformers Outperform Older Architectures\n",
    "\n",
    "Compared to RNNs and LSTMs:  \n",
    "- ‚úÖ Process text **in parallel** instead of sequentially  \n",
    "- ‚úÖ Handle **long-range dependencies** better  \n",
    "- ‚úÖ Scale efficiently to billions of parameters  \n",
    "\n",
    "### üìù Example Comparisons\n",
    "| Feature                  | RNN/LSTM                | Transformer             |\n",
    "|--------------------------|--------------------------|-------------------------|\n",
    "| Processing               | Sequential               | Parallel                |\n",
    "| Long Text Handling       | Limited (vanishing gradients) | Excellent with attention |\n",
    "| Training Time            | Slower                  | Faster                  |\n",
    "| Scalability              | Hard to scale           | Scales to massive models|"
   ]
  },
  {
   "cell_type": "code",
   "id": "a4970e8c-312d-4b91-9e9f-acceca0579cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T03:01:52.238780Z",
     "start_time": "2025-12-16T03:01:35.865331Z"
    }
   },
   "source": [
    "# Prompt: Compare transformers with RNNs and LSTMs in a table with examples\n",
    "prompt = (\n",
    "    \"Compare transformers with RNNs and LSTMs in a detailed tabular format. \"\n",
    "    \"Include real-world analogies for each row.\"\n",
    ")\n",
    "\n",
    "response = connector.get_completion(prompt)\n",
    "print(response['content'] if isinstance(response, dict) else response)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content=\"Certainly! Here's a detailed comparison of Transformers, RNNs, and LSTMs in a tabular format, including real-world analogies for each aspect:\\n\\n| Feature/Aspect            | Transformers                                      | RNNs                                          | LSTMs                                         | Real-World Analogy                             |\\n|---------------------------|---------------------------------------------------|-----------------------------------------------|-----------------------------------------------|------------------------------------------------|\\n| **Architecture**          | Self-attention mechanism with feedforward layers  | Sequential processing with recurrent layers   | Similar to RNNs but with gating mechanisms    | Transformers: Team of experts discussing in parallel; RNNs: A single storyteller passing the baton; LSTMs: Storyteller with memory aids |\\n| **Handling Long Sequences** | Excellent due to self-attention                  | Struggles with very long sequences            | Better than RNNs due to gates but still limited | Transformers: Reading multiple pages at once; RNNs: Reading line by line; LSTMs: Reading with bookmarks |\\n| **Parallelization**       | Highly parallelizable, efficient for training     | Sequential processing, slow to train          | Improved over RNNs but still sequential        | Transformers: Multiple chefs cooking at once; RNNs: One chef making one dish at a time; LSTMs: Chef with a helper for certain tasks |\\n| **Memory/Context**        | Global context through self-attention             | Limited to recent past due to vanishing gradient problem | Better context retention with memory cells    | Transformers: Encyclopedia access; RNNs: Short-term memory; LSTMs: Personal diary with key points |\\n| **Complexity**            | Requires substantial computational resources      | Simpler, requires less computation            | More complex than RNNs due to gates            | Transformers: Supercomputer; RNNs: Basic calculator; LSTMs: Calculator with memory functions |\\n| **Training Data Requirements** | Requires large datasets to generalize well    | Can work with smaller datasets                | Similar to RNNs, but benefits from more data   | Transformers: Encyclopedic knowledge; RNNs: Local gossip; LSTMs: Diary entries with years of data |\\n| **Use Cases**             | NLP, translation, vision tasks                    | Language modeling, time series                | Language modeling, time series with long dependencies | Transformers: Multilingual interpreter; RNNs: Local news reporter; LSTMs: Historian with detailed notes |\\n| **Attention Mechanism**   | Self-attention allows focus on relevant parts     | No inherent attention mechanism               | No inherent attention, but can use external attention | Transformers: Spotlight on stage; RNNs: Linear narration; LSTMs: Narration with highlights |\\n| **Handling Dependencies** | Captures long-range dependencies effectively      | Struggles with long-range dependencies        | Better with dependencies due to gates          | Transformers: Seeing forest and trees; RNNs: Seeing only trees; LSTMs: Seeing trees with some forest view |\\n| **Learning Curve**        | Steep, requires understanding of complex mechanisms | Moderate, straightforward sequential logic    | Moderate, requires understanding of gate operations | Transformers: Learning astrophysics; RNNs: Learning basic storytelling; LSTMs: Learning storytelling with memory tricks |\\n\\nThis table presents a structured comparison of the three architectures, highlighting their strengths and weaknesses along with real-world analogies to provide a clearer understanding of their functionalities and applications.\", refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "be6096b0-06e4-423a-a403-dd66993e024c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üèõÔ∏è Popular LLM Architectures\n",
    "\n",
    "### GPT (Generative Pre-trained Transformer)\n",
    "- **Type**: Decoder-only\n",
    "- **Use Case**: Text generation, completion\n",
    "- **Key Features**: Autoregressive generation, large scale\n",
    "- **Examples**: GPT-3, GPT-4, ChatGPT\n",
    "\n",
    "### BERT (Bidirectional Encoder Representations)\n",
    "- **Type**: Encoder-only\n",
    "- **Use Case**: Understanding, classification, Q&A\n",
    "- **Key Features**: Bidirectional context, masked language modeling\n",
    "- **Examples**: BERT, RoBERTa, ALBERT\n",
    "\n",
    "### T5 (Text-to-Text Transfer Transformer)\n",
    "- **Type**: Encoder-decoder\n",
    "- **Use Case**: Translation, summarization, all NLP tasks as text-to-text\n",
    "- **Key Features**: Unified framework, all tasks as generation\n",
    "- **Examples**: T5, mT5\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Summary\n",
    "\n",
    "In this notebook, we've covered:\n",
    "\n",
    "‚úÖ **Transformers** - The revolutionary architecture enabling modern LLMs  \n",
    "‚úÖ **Attention Mechanism** - How models focus on relevant information dynamically  \n",
    "‚úÖ **Core Components** - Embeddings, positional encoding, attention, feed-forward layers  \n",
    "‚úÖ **Architecture Variants** - GPT, BERT, T5 and their use cases  \n",
    "‚úÖ **Why Transformers Win** - Advantages over RNNs and LSTMs  \n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **Transformers** process text in parallel using self-attention\n",
    "- **Attention** allows models to understand relationships between any words\n",
    "- **Architecture choice** (encoder/decoder/both) depends on the task\n",
    "- **Parallel processing** makes Transformers faster and more scalable\n",
    "- **Modern LLMs** build on Transformer foundations with scale and improvements\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **Notebook 4**: Understand training vs inference differences\n",
    "- **Notebook 5**: Learn prompt engineering techniques\n",
    "- **Notebook 8**: Explore fine-tuning for specific architectures\n",
    "\n",
    "---\n",
    "\n",
    "## üéì Try It Yourself!\n",
    "\n",
    "**Exercise 1**: Think about a sentence and identify which words should have high attention when processing each word.\n",
    "\n",
    "**Exercise 2**: Research the differences between GPT-3 and GPT-4 architectures. What improvements were made?\n",
    "\n",
    "**Exercise 3**: Compare BERT and GPT. When would you use each?\n",
    "\n",
    "**Exercise 4**: Design an attention visualization for a simple sentence. Which words should attend to which?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
