{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f51fe5e-bdf1-430c-bd30-077f397c235f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# üìñ Section 1: Introduction to Large Language Models (LLMs)\n",
    "\n",
    "Large Language Models (LLMs) like OpenAI‚Äôs GPT-4 and Azure OpenAI‚Äôs GPT-35-turbo are redefining how humans interact with machines. They can generate text, translate languages, summarize documents, write code, and even solve problems.  \n",
    "\n",
    "In this notebook, we‚Äôll explore:  \n",
    "‚úÖ What LLMs are  \n",
    "‚úÖ Their capabilities with real examples  \n",
    "‚úÖ Why they matter in modern AI  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abb6f5e2-f6cd-49fc-89f9-7072a7a2c85c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîë LLM Configuration Check:\n",
      "‚úÖ Azure API Details: FOUND\n",
      "‚úÖ Connected to Azure OpenAI (deployment: gpt-4o)\n",
      "üì° LLM Connector initialized.\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# üìì SECTION 1: INTRODUCTION TO LLMs\n",
    "# =============================\n",
    "\n",
    "%run ./utils_llm_connector.ipynb\n",
    "\n",
    "# Create an instance of the connector\n",
    "connector = LLMConnector()\n",
    "\n",
    "# Confirm LLM is ready\n",
    "print(\"üì° LLM Connector initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61dbd9e-01d8-4193-82bc-81ea039d8e1a",
   "metadata": {},
   "source": [
    "## üîç What is a Large Language Model (LLM)?\n",
    "\n",
    "A **Large Language Model** is a type of artificial intelligence trained on massive amounts of text data. It learns the patterns and relationships between words and sentences to generate new, human-like text.  \n",
    "\n",
    "Think of it as a super-intelligent autocomplete system‚Äîbut one that can hold conversations, write essays, or even debug code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5fffdb2e-d5ce-4df7-997e-a0310f5c9aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content='Sure! Here\\'s a simple explanation:\\n\\nA **Large Language Model (LLM)** is a type of computer program that can understand and generate text, like answering questions, writing essays, or having conversations. It has been trained on a huge amount of text (like books, websites, and articles) so it can learn patterns of how words and sentences work together.\\n\\n### How it works:\\n1. **Training**: The LLM reads a lot of text and learns relationships between words, phrases, and ideas over time.\\n2. **Prediction**: When you give it a prompt (like a question or a sentence), it predicts what comes next or generates a response based on what it learned during training.\\n\\n### Analogy:\\nThink of an LLM like a super-smart autocomplete on your phone. When you type a message, your phone guesses the next word based on what you\\'ve already typed. An LLM does something similar, but it\\'s much better at understanding context and generating long, meaningful replies, like writing an entire story or answering complex questions.\\n\\nSo, imagine you‚Äôre talking to a robot that has read millions of books‚Äîit doesn‚Äôt \"know\" everything like a human does, but it‚Äôs really good at using patterns from all that reading to create answers that sound intelligent!', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "# Prompt: Explain LLMs simply\n",
    "prompt = (\n",
    "    \"In very simple terms, explain what a Large Language Model (LLM) is, \"\n",
    "    \"how it works, and give an analogy a non-technical person can understand.\"\n",
    ")\n",
    "\n",
    "response = connector.get_completion(prompt)\n",
    "print(response['content'] if isinstance(response, dict) else response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e8f01f-723f-47a5-a568-2ee2f06a22fd",
   "metadata": {},
   "source": [
    "### üìù Real-world Analogies for LLMs\n",
    "\n",
    "Here are 5 analogies to help understand LLMs:  \n",
    "\n",
    "1. üß† **LLM as a Brain**: Like a brain trained on the entire internet, predicting what words come next.  \n",
    "2. üéπ **LLM as a Pianist**: A pianist who‚Äôs practiced every song ever and can improvise new ones.  \n",
    "3. üç≥ **LLM as a Chef**: A chef who can create new recipes after tasting millions of dishes.  \n",
    "4. üìö **LLM as an Encyclopedia**: A dynamic encyclopedia that writes new articles from what it knows.  \n",
    "5. üó∫Ô∏è **LLM as a Map**: Predicts the next step in a journey based on past travels.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3f16aa-c499-4329-b61c-66b8ce1dda8f",
   "metadata": {},
   "source": [
    "## üöÄ Capabilities of LLMs\n",
    "\n",
    "LLMs are powerful because they can:  \n",
    "\n",
    "1. ‚úçÔ∏è **Generate Text**: Write articles, poems, or code.  \n",
    "2. üåê **Translate Languages**: Translate between hundreds of languages.  \n",
    "3. üìñ **Summarize Content**: Condense long documents into key points.  \n",
    "4. ü§ñ **Answer Questions**: Act as a virtual assistant answering queries.  \n",
    "5. üß† **Reason and Plan**: Solve problems, brainstorm, and even plan events.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "751a9d5e-cb84-42ed-aadd-ad21bb88ae2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content='Large Language Models (LLMs) like GPT are highly versatile and can perform a wide range of tasks. Below are five major capabilities of LLMs, along with simple real-life examples to illustrate each:\\n\\n---\\n\\n### 1. **Natural Language Understanding**\\nLLMs can comprehend and interpret human language, including syntax, semantics, and context.\\n\\n**Example**:  \\nYou want to know the meaning of a sentence or phrase.  \\n- **Scenario**: You ask, \"What does \\'break the ice\\' mean?\"  \\n- **LLM Response**: Explains that \"break the ice\" means initiating conversation or easing tension in a social setting.\\n\\n---\\n\\n### 2. **Generating Human-like Text**\\nLLMs can produce coherent and contextually relevant text in response to prompts.\\n\\n**Example**:  \\nYou need to write an email but don\\'t know where to start.  \\n- **Scenario**: \"Write an email to my professor asking for an extension on an assignment.\"  \\n- **LLM Response**: Generates a polite and well-structured email:  \\n  *\"Dear Professor [Name], I hope this message finds you well. I am writing to kindly request an extension on the assignment due [date]. Due to unforeseen circumstances, I need additional time to complete it. Thank you for considering my request. Best regards, [Your Name].\"*\\n\\n---\\n\\n### 3. **Answering Questions and Providing Information**\\nLLMs can retrieve or infer answers based on their training and contextual understanding.\\n\\n**Example**:  \\nYou want to know how to bake a cake.  \\n- **Scenario**: \"How do I bake a simple vanilla cake?\"  \\n- **LLM Response**: Provides a step-by-step recipe, including ingredients and instructions for mixing, baking, and cooling the cake.\\n\\n---\\n\\n### 4. **Summarizing and Condensing Information**\\nLLMs can condense long texts into shorter summaries while retaining key information.\\n\\n**Example**:  \\nYou need a summary of a news article but don‚Äôt have time to read the whole thing.  \\n- **Scenario**: You paste a long article about climate change and ask, \"Can you summarize this?\"  \\n- **LLM Response**: Provides a concise summary: *\"The article discusses the impacts of climate change, including rising global temperatures, extreme weather events, and the importance of international cooperation to reduce greenhouse gas emissions.\"*\\n\\n---\\n\\n### 5. **Language Translation**\\nLLMs can translate text from one language to another with high accuracy.\\n\\n**Example**:  \\nYou need to translate a sentence into Spanish.  \\n- **Scenario**: \"Translate \\'Where is the nearest hospital?\\' into Spanish.\"  \\n- **LLM Response**: *\"¬øD√≥nde est√° el hospital m√°s cercano?\"*\\n\\n---\\n\\nThese examples showcase the practical applications of LLMs in everyday life, ranging from communication assistance to knowledge retrieval and creative tasks.', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "# Prompt: List LLM capabilities with real-life examples\n",
    "prompt = (\n",
    "    \"List and explain 5 major capabilities of Large Language Models (LLMs) \"\n",
    "    \"with a simple real-life example for each.\"\n",
    ")\n",
    "\n",
    "response = connector.get_completion(prompt)\n",
    "print(response['content'] if isinstance(response, dict) else response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89328b52-b97d-42bb-8afb-2025f529eadb",
   "metadata": {},
   "source": [
    "## üî• LLMs vs Traditional Machine Learning Models\n",
    "\n",
    "| Feature                 | Traditional ML             | Large Language Models (LLMs)      |\n",
    "|-------------------------|----------------------------|------------------------------------|\n",
    "| üìö Training Data        | Small, task-specific data  | Massive text datasets (entire internet) |\n",
    "| üß† Capabilities         | Narrow (one task)          | Broad (multi-task, reasoning)     |\n",
    "| üìù Example              | Spam detection model       | ChatGPT answering complex questions |\n",
    "| üèóÔ∏è Adaptability         | Needs retraining per task  | Zero-shot/few-shot learning       |\n",
    "| üöÄ Real-world Impact    | Limited to domain          | Cross-domain applications         |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88b1a354-eeaf-47a8-aba7-0914193affc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content='Here‚Äôs a detailed yet easy-to-read tabular comparison of Large Language Models (LLMs) and traditional Machine Learning models:\\n\\n| **Aspect**                | **Large Language Models (LLMs)**                                     | **Traditional Machine Learning Models**                               |\\n|---------------------------|----------------------------------------------------------------------|----------------------------------------------------------------------|\\n| **Definition**            | LLMs are deep learning models trained on vast amounts of text data to understand and generate human-like language. | Traditional models include algorithms like linear regression, SVM, decision trees, etc., focused on specific tasks. |\\n| **Data Input**            | Text data, often unstructured, such as documents, websites, and chats. | Structured data (e.g., tables, numerical features) and sometimes unstructured data. |\\n| **Task Scope**            | General-purpose, capable of handling diverse NLP tasks (e.g., translation, summarization, question answering). | Specialized for narrow tasks like classification, regression, clustering, etc. |\\n| **Training Data Size**    | Requires enormous datasets (hundreds of gigabytes to terabytes) for pretraining. | Requires smaller datasets, typically in megabytes to gigabytes range. |\\n| **Model Complexity**      | Extremely complex architectures, often billions of parameters (e.g., GPT, BERT). | Simpler architectures, typically thousands to millions of parameters. |\\n| **Pretraining**           | Pretrained on large-scale corpora; fine-tuning is optional for specific tasks. | No pretraining; models are directly trained on task-specific data. |\\n| **Generalization**        | Strong generalization across tasks due to pretraining on diverse data. | Limited generalization; tailored to specific tasks and datasets. |\\n| **Resource Requirements** | High computational requirements for training and inference (e.g., GPUs/TPUs). | Lower computational requirements; can often run on CPUs. |\\n| **Interpretability**      | Difficult to interpret due to complex architectures and massive parameter count. | Easier to interpret, especially for simpler models like linear regression or decision trees. |\\n| **Scalability**           | Scales well for large-scale data but can be expensive and challenging to deploy. | Scales efficiently for smaller datasets; cost-effective for simple applications. |\\n| **Usage**                 | Ideal for tasks requiring human-like understanding of language (e.g., chatbots, content generation). | Best suited for structured tasks like predicting numerical values, customer segmentation, etc. |\\n| **Domain Adaptability**   | Can adapt to new domains with fine-tuning or prompt engineering. | Requires retraining or feature engineering to adapt to new domains. |\\n| **Learning Paradigm**     | Primarily unsupervised (pretraining) with optional supervised fine-tuning. | Typically supervised or unsupervised learning directly on task-specific data. |\\n| **Examples**              | GPT (OpenAI), BERT (Google), PaLM (Google), Claude (Anthropic).       | Logistic regression, random forests, k-means clustering, XGBoost.   |\\n| **Strengths**             | Versatile, human-like language generation, context awareness, multitasking. | Simpler implementation, lower resource needs, interpretable results. |\\n| **Weaknesses**            | Resource-intensive, less interpretable, potential ethical concerns (e.g., bias). | Limited to narrow tasks, requires domain-specific feature engineering. |\\n\\nThis table provides a clear comparison between LLMs and traditional machine learning models, highlighting their differences, strengths, and weaknesses.', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "# Prompt: Compare LLMs vs Traditional ML\n",
    "prompt = (\n",
    "    \"Compare Large Language Models (LLMs) with traditional Machine Learning models \"\n",
    "    \"in a detailed but easy-to-read tabular format.\"\n",
    ")\n",
    "\n",
    "response = connector.get_completion(prompt)\n",
    "print(response['content'] if isinstance(response, dict) else response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239dc78f-79d5-4f48-bc70-29b0ba917e3c",
   "metadata": {},
   "source": [
    "## ‚úÖ Summary\n",
    "\n",
    "In this section, we:  \n",
    "- Explored what LLMs are and how they work.  \n",
    "- Saw real-world analogies to make it relatable.  \n",
    "- Discussed LLM capabilities with live examples.  \n",
    "- Compared LLMs to traditional ML models.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
